{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f8369d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 09:12:56.120611: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 09:12:56.139682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 09:12:56.420527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from configs.config import configs\n",
    "\n",
    "from models.encoder import res_encoderS, res_encoderXS\n",
    "\n",
    "from models.classifier import transformer_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd69da",
   "metadata": {},
   "source": [
    "### print result of norm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c757178e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "conv_layer = nn.Conv1d(19, 19*2, kernel_size=3, groups=19, stride=2, padding=3, bias=False)\n",
    "print(conv_layer.weight,torch.norm(conv_layer.weight))\n",
    "nn.init.xavier_normal_(conv_layer.weight)\n",
    "print(conv_layer.weight, torch.norm(conv_layer.weight, dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320dd3e",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb628e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: %0.95\n",
      "INFO:__main__:Current accuracy: %0.95\n"
     ]
    }
   ],
   "source": [
    "# Init logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "acc_example = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: %{acc_example}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535cc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderS+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60dbe698",
   "metadata": {},
   "source": [
    "if Configs['checkpoint']['checkpoint_dir'] is not None:\n",
    "    print(Configs['checkpoint']['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3852c0",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b034725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform signal\n",
    "def transform(data:Tensor, mean:Tensor, std:Tensor):\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e176c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir:str, label_dir:str, label_dict:dict, mean: list, std: list, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/seg_csv/train'\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(self.label_dir)\n",
    "        self.label_dict = label_dict\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "#         label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\n",
    "        label = self.annotations.loc[self.annotations['csv_file'] == file_name, ['label']].to_string(index=False,header=False)\n",
    "        label = torch.tensor(int(self.label_dict[label]))\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data, self.mean, self.std)\n",
    "            \n",
    "        return (data.t(), label, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6de7e7",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f879c9b",
   "metadata": {},
   "source": [
    "#### auto-encoder+transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da34d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = res_encoderS(n_channels=n_channels, groups=n_channels, num_classes=classes, \n",
    "                               len_feature=input_size, d_model=model_hyp['d_model'])\n",
    "#         self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "#         z = x.transpose(-1,-2)\n",
    "        z = self.ae(x)\n",
    "#         z = torch.flatten(z, 1)\n",
    "#         y = self.mlp(z)\n",
    "        y = self.transformer_encoder(z)\n",
    "        return y\n",
    "        \n",
    "classifier = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes'])).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b2aca",
   "metadata": {},
   "source": [
    "#### EEGNET"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a20d7abb",
   "metadata": {},
   "source": [
    "# fs= 200                  #sampling frequency\n",
    "# channel= 22              #number of electrode\n",
    "# num_input= 1             #number of channel picture (for EEG signal is always : 1)\n",
    "# num_class= 5             #number of classes \n",
    "# signal_length = 200      #number of sample in each tarial\n",
    "\n",
    "# F1= 8                    #number of temporal filters\n",
    "# D= 3                     #depth multiplier (number of spatial filters)\n",
    "# F2= D*F1                 #number of pointwise filters\n",
    "\n",
    "# kernel_size_1= (1,round(fs/2)) \n",
    "# kernel_size_2= (channel, 1)\n",
    "# kernel_size_3= (1, round(fs/8))\n",
    "# kernel_size_4= (1, 1)\n",
    "\n",
    "# kernel_avgpool_1= (1,4)\n",
    "# kernel_avgpool_2= (1,8)\n",
    "# dropout_rate= 0.2\n",
    "\n",
    "# ks0= int(round((kernel_size_1[0]-1)/2))\n",
    "# ks1= int(round((kernel_size_1[1]-1)/2))\n",
    "# kernel_padding_1= (ks0, ks1-1)\n",
    "# ks0= int(round((kernel_size_3[0]-1)/2))\n",
    "# ks1= int(round((kernel_size_3[1]-1)/2))\n",
    "# kernel_padding_3= (ks0, ks1)\n",
    "\n",
    "class EEGNet(nn.Module): \n",
    "    def __init__(self, signal_length: int = 12000, \n",
    "                 channel: int = 19, \n",
    "                 fs:int = 100,\n",
    "                 num_class: int = 2,\n",
    "                 F1:int = 8,\n",
    "                 D:int = 3,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        num_input = 1\n",
    "        kernel_size_1= (1,round(fs/2))\n",
    "        kernel_size_2= (channel, 1)\n",
    "        kernel_size_3= (1, round(fs/8))\n",
    "        kernel_size_4= (1, 1)\n",
    "\n",
    "        kernel_avgpool_1= (1,4)\n",
    "        kernel_avgpool_2= (1,8)\n",
    "        dropout_rate= 0.2\n",
    "\n",
    "        ks0= int(round((kernel_size_1[0]-1)/2))\n",
    "        ks1= int(round((kernel_size_1[1]-1)/2))\n",
    "        kernel_padding_1= (ks0, ks1-1)\n",
    "        ks0= int(round((kernel_size_3[0]-1)/2))\n",
    "        ks1= int(round((kernel_size_3[1]-1)/2))\n",
    "        kernel_padding_3= (ks0, ks1)\n",
    "        F2= D*F1\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv2d = nn.Conv2d(num_input, F1, kernel_size_1, padding=kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(F1)\n",
    "        # layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(F1, D*F1, kernel_size_2, groups= F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(D*F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(dropout_rate)\n",
    "        # layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d( D*F1, D*F1, kernel_size_3,\n",
    "                                                padding=kernel_padding_3, groups= D*F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(D*F1, F2, kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(kernel_avgpool_2)\n",
    "        # layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.Dense = nn.Linear(F2*round(signal_length/32), num_class)\n",
    "        self.Softmax = nn.Softmax(dim= 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(-1,-2)\n",
    "        # layer 1\n",
    "        y = self.Batch_normalization_1(self.conv2d(x)) #.relu()\n",
    "        # layer 2\n",
    "        y = self.Batch_normalization_2(self.Depthwise_conv2D(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_1(y))\n",
    "        # layer 3\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Batch_normalization_3(self.Separable_conv2D_point(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_2(y))\n",
    "        # layer 4\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "        y = self.Softmax(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "classifier = EEGNet(signal_length=Configs['input_size'],channel=Configs['n_channels'],\n",
    "                   fs=Configs['processing']['frequency'],\n",
    "                    num_class=len(Configs['dataset']['classes'])\n",
    "                   ).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb10f1ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (ae): AutoEncoder(\n",
       "    (conv1): Conv1d(19, 76, kernel_size=(64,), stride=(2,), padding=(3,), groups=19, bias=False)\n",
       "    (avgpool1d): AdaptiveAvgPool1d(output_size=6000)\n",
       "    (ln1): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avgpool_1): AvgPool1d(kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln2): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 152, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(152, 152, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln2): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(76, 152, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(152, 304, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(304, 304, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln2): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(152, 304, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(304, 608, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(608, 608, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln2): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(304, 608, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool_2): AdaptiveAvgPool2d(output_size=(19, 256))\n",
       "    (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    (dropout5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): transformer_classifier(\n",
       "    (encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=4864, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67093611",
   "metadata": {},
   "source": [
    "\n",
    "for param in classifier.ae.layer1[0].conv2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35257f90",
   "metadata": {},
   "source": [
    "# test model\n",
    "batch = 16\n",
    "d_model = 19\n",
    "max_len = 12000\n",
    "x = torch.rand(batch, max_len, d_model)\n",
    "pe = model(input_size=Configs['input_size'],\n",
    "           n_channels = Configs['n_channels'],\n",
    "           model_hyp=Configs['model'],\n",
    "           classes=len(Configs['dataset']['classes']))\n",
    "           \n",
    "out = pe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e7804f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/.local/lib/python3.8/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = Configs['dataset']['train_data_dir']\n",
    "train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "val_data_dir = Configs['dataset']['val_data_dir']\n",
    "val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "label_dict = Configs['dataset']['classes']\n",
    "\n",
    "mean =  Configs['dataset']['mean']\n",
    "std =  Configs['dataset']['std']\n",
    "\n",
    "train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                              label_dir=train_label_dir,\n",
    "                              label_dict=label_dict,\n",
    "                              mean=mean,\n",
    "                              std=std,\n",
    "                             transform=transform)\n",
    "val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                            label_dir=val_label_dir,\n",
    "                            label_dict=label_dict,\n",
    "                            mean=mean,\n",
    "                            std=std,\n",
    "                           transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4,\n",
    "                              shuffle=Configs['dataset']['shuffle'], \n",
    "                          num_workers=Configs['dataset']['num_workers'], pin_memory=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset=val_dataset, num_workers=Configs['dataset']['num_workers'], \n",
    "                         shuffle=Configs['dataset']['shuffle'], pin_memory=True)\n",
    "\n",
    "# classifier = model(input_size=Configs['input_size'],\n",
    "#                                     n_channels = Configs['n_channels'],\n",
    "#                                     model_hyp=Configs['model'],\n",
    "#                                     classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "\n",
    "# input_layer = nn.Sequential(\n",
    "# #         nn.Embedding(num_embeddings=10000, embedding_dim=512),\n",
    "# #         PositionalEncoding(d_model=512, dropout=0.1, max_len=5000)\n",
    "#     PositionalEncoding(d_model=Configs['n_channels'], max_len=Configs['input_size'])\n",
    "# ).to('cuda')\n",
    "\n",
    "# classifier = model(input_size=Configs['input_size'],\n",
    "#                                         n_channels = Configs['n_channels'],\n",
    "#                                         model_hyp=Configs['model'],\n",
    "#                                         classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.999),lr=Configs['optimizer']['init_lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(Configs['tensorboard']['runs_dir']+'train_board')    # Initilize tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe786523",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dir(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dec18d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaaaapmg_s001_t000_8.csv', 'aaaaamad_s008_t000_1.csv', 'aaaaamrz_s001_t000_5.csv', 'aaaaaogs_s001_t001_6.csv']\n"
     ]
    }
   ],
   "source": [
    "iter_train = iter(train_loader)\n",
    "data, target, file = next(iter_train)\n",
    "# data = torch.randn(1, 19, 12000)\n",
    "data = data.to('cuda')\n",
    "target = target.to('cuda')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb3ad59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 19, 12000]), torch.Size([4]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f669cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:models.encoder:data shape is: torch.Size([4, 19, 12000])\n",
      "DEBUG:models.encoder:data shape after conv1: torch.Size([4, 76, 5972])\n",
      "DEBUG:models.encoder:data shape after ln1: torch.Size([4, 76, 6000])\n",
      "DEBUG:models.encoder:data shape after avgpool: torch.Size([4, 76, 3000])\n",
      "DEBUG:models.encoder:layer1\n",
      "DEBUG:models.encoder:data shape after sub-conv1: torch.Size([4, 76, 2987]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln1: torch.Size([4, 76, 3000])\n",
      "DEBUG:models.encoder:data shape after sub-relu1: torch.Size([4, 76, 3000])\n",
      "DEBUG:models.encoder:data shape after sub-conv2: torch.Size([4, 76, 2987]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln2: torch.Size([4, 76, 3000])\n",
      "DEBUG:models.encoder:data shape after residual relu+res: torch.Size([4, 76, 3000])\n",
      "DEBUG:models.encoder:layer2\n",
      "DEBUG:models.encoder:data shape after sub-conv1: torch.Size([4, 152, 1494]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln1: torch.Size([4, 152, 1500])\n",
      "DEBUG:models.encoder:data shape after sub-relu1: torch.Size([4, 152, 1500])\n",
      "DEBUG:models.encoder:data shape after sub-conv2: torch.Size([4, 152, 1487]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln2: torch.Size([4, 152, 1500])\n",
      "DEBUG:models.encoder:data shape after downsample: torch.Size([4, 152, 1500]), groups=19\n",
      "DEBUG:models.encoder:data shape after residual relu+res: torch.Size([4, 152, 1500])\n",
      "DEBUG:models.encoder:layer3\n",
      "DEBUG:models.encoder:data shape after sub-conv1: torch.Size([4, 304, 744]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln1: torch.Size([4, 304, 750])\n",
      "DEBUG:models.encoder:data shape after sub-relu1: torch.Size([4, 304, 750])\n",
      "DEBUG:models.encoder:data shape after sub-conv2: torch.Size([4, 304, 737]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln2: torch.Size([4, 304, 750])\n",
      "DEBUG:models.encoder:data shape after downsample: torch.Size([4, 304, 750]), groups=19\n",
      "DEBUG:models.encoder:data shape after residual relu+res: torch.Size([4, 304, 750])\n",
      "DEBUG:models.encoder:layer4\n",
      "DEBUG:models.encoder:data shape after sub-conv1: torch.Size([4, 608, 369]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln1: torch.Size([4, 608, 375])\n",
      "DEBUG:models.encoder:data shape after sub-relu1: torch.Size([4, 608, 375])\n",
      "DEBUG:models.encoder:data shape after sub-conv2: torch.Size([4, 608, 362]), groups=19\n",
      "DEBUG:models.encoder:data shape after sub-ln2: torch.Size([4, 608, 375])\n",
      "DEBUG:models.encoder:data shape after downsample: torch.Size([4, 608, 375]), groups=19\n",
      "DEBUG:models.encoder:data shape after residual relu+res: torch.Size([4, 608, 375])\n",
      "DEBUG:models.encoder:data shape after avgpool: torch.Size([4, 19, 256])\n",
      "DEBUG:models.classifier:transformer output size: torch.Size([4, 19, 256])\n",
      "DEBUG:models.classifier:flatten output size: torch.Size([4, 4864])\n",
      "DEBUG:models.classifier:linear output size: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "out = classifier(data)\n",
    "loss = criterion(out, target)\n",
    "probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "_, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "# flops = FlopCountAnalysis(classifier, data)\n",
    "# print(\"FLOPs:\", flops.total())\n",
    "# print(parameter_count_table(classifier))\n",
    "# print(out)\n",
    "# print(probabilities)\n",
    "# print(target)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec638b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier.ae"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a44d9a5e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "input_pe = input_layer(data)\n",
    "out = classifier(input_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803bab0",
   "metadata": {},
   "source": [
    "#### Check eval results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "029fba4b",
   "metadata": {},
   "source": [
    "model_para = torch.load('weights/resnet18_best.pth')\n",
    "classifier.load_state_dict(model_para['model_state_dict'])\n",
    "classifier.cuda().eval()\n",
    "\n",
    "train_signal = iter(train_loader)\n",
    "eval_signal = iter(eval_loader)\n",
    "\n",
    "train_data = next(train_signal)\n",
    "\n",
    "x_train, y_train = train_data[0].to('cuda'), train_data[1].to('cuda')\n",
    "train_pred = classifier(x_train)\n",
    "train_loss = criterion(train_pred, y_train)\n",
    "train_loss\n",
    "\n",
    "eval_data = next(eval_signal)\n",
    "\n",
    "x_eval, y_eval = eval_data[0].to('cuda'), eval_data[1].to('cuda')\n",
    "eval_pred = classifier(x_eval)\n",
    "eval_loss = criterion(eval_pred, y_eval)\n",
    "eval_loss\n",
    "\n",
    "train_pred, eval_pred\n",
    "y_train, y_eval\n",
    "\n",
    "_, predicted = torch.max(eval_pred, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8add33de",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Validate results\n",
    "\n",
    "import torch.nn.functional as F\n",
    "val_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    outputs = classifier(data)\n",
    "    print(_)\n",
    "    loss = criterion(outputs, target)\n",
    "    val_loss += loss.item()\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    print(loss.item(), probabilities)\n",
    "    _, predicted = torch.max(probabilities, 1)\n",
    "    print(predicted, target)\n",
    "    total += target.size(0)  # Total number of samples\n",
    "    correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "    print(correct, total)\n",
    "    val_loss /= len(eval_loader)\n",
    "    val_accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning rate update policy\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.info(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e972650",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c11398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Configs:dict):\n",
    "    train_data_dir = Configs['dataset']['train_data_dir']\n",
    "    train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "    val_data_dir = Configs['dataset']['val_data_dir']\n",
    "    val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "    label_dict = Configs['dataset']['classes']\n",
    "    train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                                  label_dir=train_label_dir,\n",
    "                                  label_dict=label_dict)\n",
    "    val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                                label_dir=val_label_dir,\n",
    "                                label_dict=label_dict)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=Configs['train']['batch_size'],\n",
    "                                  shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "    eval_loader = DataLoader(dataset=val_dataset, num_workers=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "    classifier = transformer_classifier(input_size=Configs['input_size'], \n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.9),lr=Configs['optimizer']['init_lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(Configs['tensorboard']['runs_dir']+'train_board')    # Initilize tensorflow\n",
    "    \n",
    "    ### Warmup training\n",
    "    warmup_steps = Configs['train']['warmup_steps']\n",
    "    warmup_step = 0\n",
    "    min_loss = 1\n",
    "    while warmup_step < warmup_steps:\n",
    "        for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "            if warmup_step < warmup_steps:\n",
    "        #     for batch_index, data in enumerate(train_loader, 0):\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "                y = classifier(data)\n",
    "        #         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "                warmup_loss = criterion(y, target)\n",
    "                optimizer.zero_grad()\n",
    "                warmup_loss.backward()\n",
    "                optimizer.step()\n",
    "        #         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "                logger.info(f\"Warmup Step: {warmup_step}, Warmup Loss: {warmup_loss}\")\n",
    "                writer.add_scalar('Warmup Loss', warmup_loss, global_step=warmup_step)\n",
    "                warmup_step += 1\n",
    "        \n",
    "#         if warmup_step%5==0:\n",
    "#             torch.save(classifier.state_dict(), \n",
    "#                        Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_last.pth')\n",
    "        if warmup_loss < min_loss:\n",
    "            torch.save(classifier.state_dict(),\n",
    "                       Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_best.pth')\n",
    "            min_loss = warmup_loss\n",
    "        \n",
    "    \n",
    "    ### train\n",
    "#     step = 0\n",
    "#     epochs = Configs['train']['n_epochs']\n",
    "#     for epoch in range(epochs):\n",
    "#         # Training loop\n",
    "#         poly_lr_scheduler(optimizer, init_lr=Configs['optimizer']['init_lr'], iter=epoch, max_iter=epochs)\n",
    "#         for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "#             data, target = data.to('cuda'), target.to('cuda')\n",
    "#             y = classifier(data)\n",
    "#     #         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "#             train_loss = criterion(y, target)\n",
    "#             optimizer.zero_grad()\n",
    "#             train_loss.backward()\n",
    "#             optimizer.step()\n",
    "#     #         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "#             logger.info(f\"Step: {step}, training Loss: {train_loss}\")\n",
    "#             writer.add_scalar('Training Loss', train_loss, global_step=step)\n",
    "#             step += 1\n",
    "        \n",
    "#         if epoch%5==0:\n",
    "#             val_loss = 0\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             accuracy = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "#                     data, target = data.to('cuda'), target.to('cuda')\n",
    "#                     outputs = classifier(data)\n",
    "#                     loss = criterion(outputs, target)\n",
    "#                     val_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs, 1)\n",
    "#                     total += target.size(0)  # Total number of samples\n",
    "#                     correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "\n",
    "#             val_loss /= len(eval_loader)\n",
    "#             accuracy = 100 * correct / total\n",
    "#             writer.add_scalar('Validation Loss', val_loss, global_step=step)\n",
    "#             writer.add_scalar('Validation Accuracy', accuracy, global_step=step)\n",
    "#             logger.info(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "#         torch.save(classifier.state_dict(), \n",
    "#                    Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_latest.pth')\n",
    "#         if train_loss < min_loss:\n",
    "#             torch.save(classifier.state_dict(),\n",
    "#                        Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_best.pth')\n",
    "#             min_loss = train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf605cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python38]",
   "language": "python",
   "name": "conda-env-python38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
