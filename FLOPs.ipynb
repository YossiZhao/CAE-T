{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4bee52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "%matplotlib notebook\n",
    "\n",
    "from models.encoder2 import res_encoderS, res_encoderM\n",
    "from models.classifier import transformer_classifier\n",
    "from models.EEGNET import EEGNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb9f5779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: %0.95\n",
      "INFO:__main__:Current accuracy: %0.95\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "acc_example = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: %{acc_example}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ebef9",
   "metadata": {},
   "source": [
    "### FLOPs for auto-encoderS + transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f98c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderS+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "436c7275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (ae): AutoEncoder(\n",
       "    (conv1): Conv1d(19, 76, kernel_size=(64,), stride=(2,), padding=(3,), groups=19, bias=False)\n",
       "    (avgpool1d): AdaptiveAvgPool1d(output_size=6000)\n",
       "    (ln1): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avgpool_1): AvgPool1d(kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 152, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(76, 152, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(152, 304, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(152, 304, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(304, 608, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(304, 608, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool_2): AdaptiveAvgPool2d(output_size=(19, 256))\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): transformer_classifier(\n",
       "    (encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=4864, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = res_encoderS(n_channels=n_channels, groups=n_channels, num_classes=classes, \n",
    "                               len_feature=input_size, d_model=model_hyp['d_model'])\n",
    "#         self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "#         z = x.transpose(-1,-2)\n",
    "        z = self.ae(x)\n",
    "#         z = torch.flatten(z, 1)\n",
    "#         y = self.mlp(z)\n",
    "        y = self.transformer_encoder(z)\n",
    "        return y\n",
    "        \n",
    "classifier_s = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes']))\n",
    "classifier_s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b4134cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = torch.randn(1, 19, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b7e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::adaptive_avg_pool1d encountered 6 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool1d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 5 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 4 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "transformer_encoder.encoder_layer, transformer_encoder.encoder_layer.dropout, transformer_encoder.encoder_layer.dropout1, transformer_encoder.encoder_layer.dropout2, transformer_encoder.encoder_layer.linear1, transformer_encoder.encoder_layer.linear2, transformer_encoder.encoder_layer.norm1, transformer_encoder.encoder_layer.norm2, transformer_encoder.encoder_layer.self_attn, transformer_encoder.encoder_layer.self_attn.out_proj, transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 201997664\n",
      "| name                                              | #elements or shape   |\n",
      "|:--------------------------------------------------|:---------------------|\n",
      "| model                                             | 2.9M                 |\n",
      "|  ae                                               |  0.3M                |\n",
      "|   ae.conv1                                        |   4.9K               |\n",
      "|    ae.conv1.weight                                |    (76, 1, 64)       |\n",
      "|   ae.ln1                                          |   12.0K              |\n",
      "|    ae.ln1.weight                                  |    (6000,)           |\n",
      "|    ae.ln1.bias                                    |    (6000,)           |\n",
      "|   ae.layers                                       |   0.2M               |\n",
      "|    ae.layers.0                                    |    21.7K             |\n",
      "|    ae.layers.1                                    |    16.3K             |\n",
      "|    ae.layers.2                                    |    44.3K             |\n",
      "|    ae.layers.3                                    |    0.2M              |\n",
      "|  transformer_encoder                              |  2.6M                |\n",
      "|   transformer_encoder.encoder_layer               |   1.3M               |\n",
      "|    transformer_encoder.encoder_layer.self_attn    |    0.3M              |\n",
      "|    transformer_encoder.encoder_layer.linear1      |    0.5M              |\n",
      "|    transformer_encoder.encoder_layer.linear2      |    0.5M              |\n",
      "|    transformer_encoder.encoder_layer.norm1        |    0.5K              |\n",
      "|    transformer_encoder.encoder_layer.norm2        |    0.5K              |\n",
      "|   transformer_encoder.transformer_encoder         |   1.3M               |\n",
      "|    transformer_encoder.transformer_encoder.layers |    1.3M              |\n",
      "|   transformer_encoder.linear                      |   9.7K               |\n",
      "|    transformer_encoder.linear.weight              |    (2, 4864)         |\n",
      "|    transformer_encoder.linear.bias                |    (2,)              |\n"
     ]
    }
   ],
   "source": [
    "# out = classifier(data_s)\n",
    "# # loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_s, data_s)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32454526",
   "metadata": {},
   "source": [
    "### FLOPs for auto-encoderM + transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25be375f",
   "metadata": {},
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderM+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fce1b7d",
   "metadata": {},
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = res_encoderM(n_channels=n_channels, groups=n_channels, num_classes=classes, \n",
    "                               len_feature=input_size, d_model=model_hyp['d_model'])\n",
    "#         self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "#         z = x.transpose(-1,-2)\n",
    "        z = self.ae(x)\n",
    "#         z = torch.flatten(z, 1)\n",
    "#         y = self.mlp(z)\n",
    "        y = self.transformer_encoder(z)\n",
    "        return y\n",
    "        \n",
    "classifier_m = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes']))\n",
    "classifier_m.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70ebe6d2",
   "metadata": {},
   "source": [
    "data_m = torch.randn(1, 19, 24000)\n",
    "out = classifier_m(data_m)\n",
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_m, data_m)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86939a",
   "metadata": {},
   "source": [
    "### FLOPs for EEGNet_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b1beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/EEGNET.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e677f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGNet(\n",
       "  (conv2d): Conv2d(1, 8, kernel_size=(1, 50), stride=(1, 1), padding=(0, 23))\n",
       "  (Batch_normalization_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Depthwise_conv2D): Conv2d(8, 24, kernel_size=(19, 1), stride=(1, 1), groups=8)\n",
       "  (Batch_normalization_2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Elu): ELU(alpha=1.0)\n",
       "  (Average_pooling2D_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (Dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (Separable_conv2D_depth): Conv2d(24, 24, kernel_size=(1, 12), stride=(1, 1), padding=(0, 6), groups=24)\n",
       "  (Separable_conv2D_point): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Batch_normalization_3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Average_pooling2D_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Dense): Linear(in_features=9000, out_features=2, bias=True)\n",
       "  (Softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_eeg_s = EEGNet(signal_length=Configs['input_size'],channel=Configs['n_channels'],\n",
    "               fs=Configs['processing']['frequency'],\n",
    "                num_class=len(Configs['dataset']['classes'])\n",
    "               )\n",
    "classifier_eeg_s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f6cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eeg_s = torch.randn(1, 12000, 19)\n",
    "data_eeg_s= data_eeg_s.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8893dd95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::elu encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool2d encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::feature_dropout encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 103624776\n",
      "| name                            | #elements or shape   |\n",
      "|:--------------------------------|:---------------------|\n",
      "| model                           | 19.9K                |\n",
      "|  conv2d                         |  0.4K                |\n",
      "|   conv2d.weight                 |   (8, 1, 1, 50)      |\n",
      "|   conv2d.bias                   |   (8,)               |\n",
      "|  Batch_normalization_1          |  16                  |\n",
      "|   Batch_normalization_1.weight  |   (8,)               |\n",
      "|   Batch_normalization_1.bias    |   (8,)               |\n",
      "|  Depthwise_conv2D               |  0.5K                |\n",
      "|   Depthwise_conv2D.weight       |   (24, 1, 19, 1)     |\n",
      "|   Depthwise_conv2D.bias         |   (24,)              |\n",
      "|  Batch_normalization_2          |  48                  |\n",
      "|   Batch_normalization_2.weight  |   (24,)              |\n",
      "|   Batch_normalization_2.bias    |   (24,)              |\n",
      "|  Separable_conv2D_depth         |  0.3K                |\n",
      "|   Separable_conv2D_depth.weight |   (24, 1, 1, 12)     |\n",
      "|   Separable_conv2D_depth.bias   |   (24,)              |\n",
      "|  Separable_conv2D_point         |  0.6K                |\n",
      "|   Separable_conv2D_point.weight |   (24, 24, 1, 1)     |\n",
      "|   Separable_conv2D_point.bias   |   (24,)              |\n",
      "|  Batch_normalization_3          |  48                  |\n",
      "|   Batch_normalization_3.weight  |   (24,)              |\n",
      "|   Batch_normalization_3.bias    |   (24,)              |\n",
      "|  Dense                          |  18.0K               |\n",
      "|   Dense.weight                  |   (2, 9000)          |\n",
      "|   Dense.bias                    |   (2,)               |\n"
     ]
    }
   ],
   "source": [
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_eeg_s, data_eeg_s)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_eeg_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c6136",
   "metadata": {},
   "source": [
    "### FLOPs for EEGNet_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f8636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/EEGNET.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8a45ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGNet(\n",
       "  (conv2d): Conv2d(1, 8, kernel_size=(1, 50), stride=(1, 1), padding=(0, 23))\n",
       "  (Batch_normalization_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Depthwise_conv2D): Conv2d(8, 24, kernel_size=(19, 1), stride=(1, 1), groups=8)\n",
       "  (Batch_normalization_2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Elu): ELU(alpha=1.0)\n",
       "  (Average_pooling2D_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (Dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (Separable_conv2D_depth): Conv2d(24, 24, kernel_size=(1, 12), stride=(1, 1), padding=(0, 6), groups=24)\n",
       "  (Separable_conv2D_point): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Batch_normalization_3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Average_pooling2D_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Dense): Linear(in_features=18000, out_features=2, bias=True)\n",
       "  (Softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_eeg_m = EEGNet(signal_length=24000,channel=Configs['n_channels'],\n",
    "               fs=Configs['processing']['frequency'],\n",
    "                num_class=len(Configs['dataset']['classes'])\n",
    "               )\n",
    "classifier_eeg_m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01eafae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eeg_m = torch.randn(1, 24000, 19)\n",
    "data_eeg_m= data_eeg_m.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d19c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::elu encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool2d encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::feature_dropout encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::softmax encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 207274776\n",
      "| name                            | #elements or shape   |\n",
      "|:--------------------------------|:---------------------|\n",
      "| model                           | 37.9K                |\n",
      "|  conv2d                         |  0.4K                |\n",
      "|   conv2d.weight                 |   (8, 1, 1, 50)      |\n",
      "|   conv2d.bias                   |   (8,)               |\n",
      "|  Batch_normalization_1          |  16                  |\n",
      "|   Batch_normalization_1.weight  |   (8,)               |\n",
      "|   Batch_normalization_1.bias    |   (8,)               |\n",
      "|  Depthwise_conv2D               |  0.5K                |\n",
      "|   Depthwise_conv2D.weight       |   (24, 1, 19, 1)     |\n",
      "|   Depthwise_conv2D.bias         |   (24,)              |\n",
      "|  Batch_normalization_2          |  48                  |\n",
      "|   Batch_normalization_2.weight  |   (24,)              |\n",
      "|   Batch_normalization_2.bias    |   (24,)              |\n",
      "|  Separable_conv2D_depth         |  0.3K                |\n",
      "|   Separable_conv2D_depth.weight |   (24, 1, 1, 12)     |\n",
      "|   Separable_conv2D_depth.bias   |   (24,)              |\n",
      "|  Separable_conv2D_point         |  0.6K                |\n",
      "|   Separable_conv2D_point.weight |   (24, 24, 1, 1)     |\n",
      "|   Separable_conv2D_point.bias   |   (24,)              |\n",
      "|  Batch_normalization_3          |  48                  |\n",
      "|   Batch_normalization_3.weight  |   (24,)              |\n",
      "|   Batch_normalization_3.bias    |   (24,)              |\n",
      "|  Dense                          |  36.0K               |\n",
      "|   Dense.weight                  |   (2, 18000)         |\n",
      "|   Dense.bias                    |   (2,)               |\n"
     ]
    }
   ],
   "source": [
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_eeg_m, data_eeg_m)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_eeg_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65f78c",
   "metadata": {},
   "source": [
    "### FLOPs for Transformer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0377d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/EEGNET.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c9664ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_classifier(nn.Module):\n",
    "    def __init__(self, input_size:int, n_channels:int, model_hyp:dict, classes:int):\n",
    "        super(transformer_classifier, self).__init__()\n",
    "\n",
    "#         self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_hyp['d_model'],\n",
    "#                                                         nhead=model_hyp['n_head'], batch_first=True)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=24000,\n",
    "                                                        nhead=1, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(model_hyp['d_model']*n_channels, classes)\n",
    "        self.linear = nn.Linear(24000*n_channels, classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.transformer_encoder(x)\n",
    "        logger.debug(f\"transformer output size: {z.shape}\")\n",
    "        z = self.flatten(z)\n",
    "        logger.debug(f\"flatten output size: {z.shape}\")\n",
    "        y = self.linear(z)\n",
    "        logger.debug(f\"linear output size: {y.shape}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9eaa58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "transformer_s= transformer_classifier(24000, 19, 12000, 2)\n",
    "transformer_s.eval()\n",
    "data_s = torch.randn(1, 19,12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "941f3d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 4 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder_layer, encoder_layer.dropout, encoder_layer.dropout1, encoder_layer.dropout2, encoder_layer.linear1, encoder_layer.linear2, encoder_layer.norm1, encoder_layer.norm2, encoder_layer.self_attn, encoder_layer.self_attn.out_proj, transformer_encoder.layers.0.self_attn.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 45649248000\n",
      "| name                                      | #elements or shape   |\n",
      "|:------------------------------------------|:---------------------|\n",
      "| model                                     | 4.8G                 |\n",
      "|  encoder_layer                            |  2.4G                |\n",
      "|   encoder_layer.self_attn                 |   2.3G               |\n",
      "|    encoder_layer.self_attn.in_proj_weight |    (72000, 24000)    |\n",
      "|    encoder_layer.self_attn.in_proj_bias   |    (72000,)          |\n",
      "|    encoder_layer.self_attn.out_proj       |    0.6G              |\n",
      "|   encoder_layer.linear1                   |   49.2M              |\n",
      "|    encoder_layer.linear1.weight           |    (2048, 24000)     |\n",
      "|    encoder_layer.linear1.bias             |    (2048,)           |\n",
      "|   encoder_layer.linear2                   |   49.2M              |\n",
      "|    encoder_layer.linear2.weight           |    (24000, 2048)     |\n",
      "|    encoder_layer.linear2.bias             |    (24000,)          |\n",
      "|   encoder_layer.norm1                     |   48.0K              |\n",
      "|    encoder_layer.norm1.weight             |    (24000,)          |\n",
      "|    encoder_layer.norm1.bias               |    (24000,)          |\n",
      "|   encoder_layer.norm2                     |   48.0K              |\n",
      "|    encoder_layer.norm2.weight             |    (24000,)          |\n",
      "|    encoder_layer.norm2.bias               |    (24000,)          |\n",
      "|  transformer_encoder                      |  2.4G                |\n",
      "|   transformer_encoder.layers              |   2.4G               |\n",
      "|    transformer_encoder.layers.0           |    2.4G              |\n",
      "|  linear                                   |  0.9M                |\n",
      "|   linear.weight                           |   (2, 456000)        |\n",
      "|   linear.bias                             |   (2,)               |\n"
     ]
    }
   ],
   "source": [
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(transformer_s, data_s)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(transformer_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211953c",
   "metadata": {},
   "source": [
    "### EEG-ARNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b8d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:59:57.665662: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 16:59:57.687386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 16:59:58.057806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from configs.config import configs\n",
    "\n",
    "# from models.pe import PositionalEncoding\n",
    "from EEG_ARNN.gcnModelST_pytorch import GCN_layer\n",
    "from EEG_ARNN.nnModelST_pytorch import zhnn\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b19807",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/EEG-ARNN.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af90b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::pad encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 7 time(s)\n",
      "Unsupported operator aten::elu_ encountered 7 time(s)\n",
      "Unsupported operator aten::feature_dropout encountered 7 time(s)\n",
      "Unsupported operator aten::add encountered 3 time(s)\n",
      "Unsupported operator aten::avg_pool2d encountered 3 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 260259000.0\n",
      "| name             | #elements or shape   |\n",
      "|:-----------------|:---------------------|\n",
      "| model            | 69.8K                |\n",
      "|  A               |  (19, 19)            |\n",
      "|  conv1           |  0.3K                |\n",
      "|   conv1.weight   |   (16, 1, 1, 16)     |\n",
      "|   conv1.bias     |   (16,)              |\n",
      "|  norm1           |  32                  |\n",
      "|   norm1.weight   |   (16,)              |\n",
      "|   norm1.bias     |   (16,)              |\n",
      "|  gconv2          |  27.6K               |\n",
      "|   gconv2.W       |   (60, 60)           |\n",
      "|   gconv2.theta   |   (12000,)           |\n",
      "|   gconv2.b       |   (1, 1, 1, 12000)   |\n",
      "|  norm2           |  32                  |\n",
      "|   norm2.weight   |   (16,)              |\n",
      "|   norm2.bias     |   (16,)              |\n",
      "|  dconv3          |  0.1K                |\n",
      "|   dconv3.weight  |   (16, 1, 1, 8)      |\n",
      "|   dconv3.bias    |   (16,)              |\n",
      "|  norm3           |  32                  |\n",
      "|   norm3.weight   |   (16,)              |\n",
      "|   norm3.bias     |   (16,)              |\n",
      "|  gconv4          |  9.6K                |\n",
      "|   gconv4.W       |   (60, 60)           |\n",
      "|   gconv4.theta   |   (3000,)            |\n",
      "|   gconv4.b       |   (1, 1, 1, 3000)    |\n",
      "|  norm4           |  32                  |\n",
      "|   norm4.weight   |   (16,)              |\n",
      "|   norm4.bias     |   (16,)              |\n",
      "|  dconv5          |  0.1K                |\n",
      "|   dconv5.weight  |   (16, 1, 1, 8)      |\n",
      "|   dconv5.bias    |   (16,)              |\n",
      "|  norm5           |  32                  |\n",
      "|   norm5.weight   |   (16,)              |\n",
      "|   norm5.bias     |   (16,)              |\n",
      "|  dconv6          |  0.3K                |\n",
      "|   dconv6.weight  |   (16, 1, 19, 1)     |\n",
      "|   dconv6.bias    |   (16,)              |\n",
      "|  pconv6          |  0.5K                |\n",
      "|   pconv6.weight  |   (32, 16, 1, 1)     |\n",
      "|   pconv6.bias    |   (32,)              |\n",
      "|  norm6           |  64                  |\n",
      "|   norm6.weight   |   (32,)              |\n",
      "|   norm6.bias     |   (32,)              |\n",
      "|  gconv7          |  6.6K                |\n",
      "|   gconv7.W       |   (60, 60)           |\n",
      "|   gconv7.theta   |   (1500,)            |\n",
      "|   gconv7.b       |   (1, 1, 1, 1500)    |\n",
      "|  norm7           |  32                  |\n",
      "|   norm7.weight   |   (16,)              |\n",
      "|   norm7.bias     |   (16,)              |\n",
      "|  linear1         |  24.0K               |\n",
      "|   linear1.weight |   (2, 12000)         |\n",
      "|   linear1.bias   |   (2,)               |\n"
     ]
    }
   ],
   "source": [
    "def preprocess_adj(adj):\n",
    "    adj = adj + np.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj)\n",
    "    return adj\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    d = np.diag(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "    a_norm = adj.dot(d).transpose().dot(d)\n",
    "    return a_norm\n",
    "\n",
    "df = pd.read_excel('./configs/init_adj_tuh.xlsx')\n",
    "Abf = df.iloc[:, 1:].values\n",
    "A = preprocess_adj(Abf)\n",
    "# A = np.ones((60,60))\n",
    "A = np.float32(A)\n",
    "A = torch.from_numpy(A).to('cpu')\n",
    "\n",
    "model = zhnn((19,12000), A).to('cpu')\n",
    "x = torch.rand(1, 1, 19, 12000)\n",
    "\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b28be",
   "metadata": {},
   "source": [
    "### FLOPs for Deep4Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91dce94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/.local/lib/python3.8/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from configs.config import configs\n",
    "\n",
    "\n",
    "from braindecode.models import Deep4Net\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747c65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/Deep4Conv.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675e5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.de = Deep4Net(in_chans=n_channels, n_classes=classes,\n",
    "                 input_window_samples=input_size,\n",
    "                 n_filters_time=25, n_filters_spat=25,\n",
    "                 stride_before_pool=True,\n",
    "                 n_filters_2=n_channels * 2,\n",
    "                 n_filters_3=n_channels * 4,\n",
    "                 n_filters_4=n_channels * 8,\n",
    "                 final_conv_length=1,\n",
    "                 add_log_softmax=False, )\n",
    "        self.linear = nn.Linear(2*437, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "        z = x.transpose(-1,-2)\n",
    "        z = self.de(z)\n",
    "        z = torch.flatten(z, 1)\n",
    "        y = self.linear(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf0103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/braindecode/models/base.py:23: UserWarning: Deep4Net: 'in_chans' is depreciated. Use 'n_chans' instead.\n",
      "  warnings.warn(\n",
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/braindecode/models/base.py:23: UserWarning: Deep4Net: 'n_classes' is depreciated. Use 'n_outputs' instead.\n",
      "  warnings.warn(\n",
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/braindecode/models/base.py:23: UserWarning: Deep4Net: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057f4311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mul encountered 4 time(s)\n",
      "Unsupported operator aten::sum encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::elu encountered 4 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 4 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "de.conv_time_spat.conv_spat, de.conv_time_spat.conv_time, de.pool_nonlin, de.pool_nonlin_2, de.pool_nonlin_3, de.pool_nonlin_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 185339797\n",
      "| name                              | #elements or shape   |\n",
      "|:----------------------------------|:---------------------|\n",
      "| model                             | 0.2M                 |\n",
      "|  de                               |  0.2M                |\n",
      "|   de.conv_time_spat               |   12.2K              |\n",
      "|    de.conv_time_spat.conv_time    |    0.3K              |\n",
      "|    de.conv_time_spat.conv_spat    |    11.9K             |\n",
      "|   de.bnorm                        |   50                 |\n",
      "|    de.bnorm.weight                |    (25,)             |\n",
      "|    de.bnorm.bias                  |    (25,)             |\n",
      "|   de.conv_2                       |   9.5K               |\n",
      "|    de.conv_2.weight               |    (38, 25, 10, 1)   |\n",
      "|   de.bnorm_2                      |   76                 |\n",
      "|    de.bnorm_2.weight              |    (38,)             |\n",
      "|    de.bnorm_2.bias                |    (38,)             |\n",
      "|   de.conv_3                       |   28.9K              |\n",
      "|    de.conv_3.weight               |    (76, 38, 10, 1)   |\n",
      "|   de.bnorm_3                      |   0.2K               |\n",
      "|    de.bnorm_3.weight              |    (76,)             |\n",
      "|    de.bnorm_3.bias                |    (76,)             |\n",
      "|   de.conv_4                       |   0.1M               |\n",
      "|    de.conv_4.weight               |    (152, 76, 10, 1)  |\n",
      "|   de.bnorm_4                      |   0.3K               |\n",
      "|    de.bnorm_4.weight              |    (152,)            |\n",
      "|    de.bnorm_4.bias                |    (152,)            |\n",
      "|   de.final_layer                  |   0.3K               |\n",
      "|    de.final_layer.conv_classifier |    0.3K              |\n",
      "|  linear                           |  1.8K                |\n",
      "|   linear.weight                   |   (2, 874)           |\n",
      "|   linear.bias                     |   (2,)               |\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 12000, 19)\n",
    "\n",
    "flops = FlopCountAnalysis(model, x)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294ee2c",
   "metadata": {},
   "source": [
    "### FLOPs for FusionCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
