{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f8369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 10:32:43.592324: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-02 10:32:43.611562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 10:32:43.890080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from configs.config import configs\n",
    "\n",
    "from models.resnet1D import *\n",
    "from models.pe import PositionalEncoding\n",
    "from models.transformer_encoder import transformer_classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd69da",
   "metadata": {},
   "source": [
    "### Evaluate norm of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a226dc1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.2840, -0.3031,  0.2760]],\n",
      "\n",
      "        [[ 0.2097,  0.1949,  0.5068]],\n",
      "\n",
      "        [[ 0.3945,  0.5274, -0.0688]],\n",
      "\n",
      "        [[-0.4057, -0.1100, -0.5093]],\n",
      "\n",
      "        [[-0.4131,  0.2999, -0.2677]],\n",
      "\n",
      "        [[-0.2820, -0.1204, -0.0899]],\n",
      "\n",
      "        [[-0.4914, -0.3471, -0.4890]],\n",
      "\n",
      "        [[-0.1306,  0.1420, -0.2110]],\n",
      "\n",
      "        [[ 0.0783, -0.3987,  0.0033]],\n",
      "\n",
      "        [[ 0.4158,  0.0796,  0.0731]],\n",
      "\n",
      "        [[ 0.1833, -0.5717,  0.1789]],\n",
      "\n",
      "        [[-0.5468, -0.2760,  0.3280]],\n",
      "\n",
      "        [[ 0.2127,  0.1528,  0.2395]],\n",
      "\n",
      "        [[ 0.4706, -0.2886, -0.5749]],\n",
      "\n",
      "        [[-0.2053,  0.0530, -0.3423]],\n",
      "\n",
      "        [[ 0.1479, -0.4263,  0.0765]],\n",
      "\n",
      "        [[ 0.3751,  0.2990,  0.4488]],\n",
      "\n",
      "        [[-0.5070, -0.3906,  0.1992]],\n",
      "\n",
      "        [[ 0.4713,  0.0247,  0.2751]],\n",
      "\n",
      "        [[-0.3753,  0.5430,  0.0852]],\n",
      "\n",
      "        [[-0.5233,  0.0917, -0.3727]],\n",
      "\n",
      "        [[ 0.3672,  0.0901, -0.0502]],\n",
      "\n",
      "        [[ 0.1158, -0.0300,  0.0212]],\n",
      "\n",
      "        [[-0.2815, -0.1430, -0.1202]],\n",
      "\n",
      "        [[-0.2230,  0.4115,  0.5476]],\n",
      "\n",
      "        [[ 0.3391, -0.4541, -0.4629]],\n",
      "\n",
      "        [[-0.2273,  0.5678, -0.4693]],\n",
      "\n",
      "        [[-0.1578, -0.0412, -0.1907]],\n",
      "\n",
      "        [[-0.2769,  0.4569, -0.5310]],\n",
      "\n",
      "        [[-0.3961, -0.3979, -0.3064]],\n",
      "\n",
      "        [[ 0.2862, -0.4386, -0.2389]],\n",
      "\n",
      "        [[-0.4153, -0.4055,  0.2765]],\n",
      "\n",
      "        [[ 0.1583,  0.4900, -0.5601]],\n",
      "\n",
      "        [[ 0.0569, -0.5291, -0.4674]],\n",
      "\n",
      "        [[ 0.5469,  0.2457,  0.5674]],\n",
      "\n",
      "        [[-0.4805,  0.0103, -0.5596]],\n",
      "\n",
      "        [[-0.3579, -0.1299,  0.0353]],\n",
      "\n",
      "        [[ 0.0076, -0.2199,  0.0996]]], requires_grad=True) tensor(3.6329, grad_fn=<LinalgVectorNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[[-5.8949e-02, -6.8291e-02,  1.6724e-01]],\n",
      "\n",
      "        [[-2.3451e-01, -6.1929e-02,  1.2053e-01]],\n",
      "\n",
      "        [[-6.0201e-02, -1.1036e-02,  1.9452e-01]],\n",
      "\n",
      "        [[-1.3468e-01, -6.2815e-02,  1.4348e-01]],\n",
      "\n",
      "        [[-1.1450e-01, -1.2430e-04, -9.7522e-02]],\n",
      "\n",
      "        [[-1.4779e-02, -1.7304e-01, -2.7979e-02]],\n",
      "\n",
      "        [[-3.6119e-02, -1.1954e-01,  5.6011e-02]],\n",
      "\n",
      "        [[ 6.7816e-02, -1.5682e-01,  6.1186e-03]],\n",
      "\n",
      "        [[-6.9367e-02, -2.5934e-02, -1.5207e-01]],\n",
      "\n",
      "        [[ 1.0789e-01,  1.1623e-01,  2.1288e-01]],\n",
      "\n",
      "        [[ 4.1603e-03,  1.5238e-01,  1.2382e-01]],\n",
      "\n",
      "        [[-9.4195e-03,  2.1609e-02,  3.1744e-02]],\n",
      "\n",
      "        [[ 1.1157e-01,  3.0509e-01,  1.0989e-01]],\n",
      "\n",
      "        [[-3.7791e-01, -6.3112e-03,  5.5459e-02]],\n",
      "\n",
      "        [[ 3.4664e-01, -8.9415e-02, -1.2299e-01]],\n",
      "\n",
      "        [[-7.2635e-02,  9.7325e-02,  6.0122e-02]],\n",
      "\n",
      "        [[-7.4422e-02,  1.3000e-01,  1.1042e-01]],\n",
      "\n",
      "        [[ 1.4311e-02,  1.6386e-01, -9.3059e-02]],\n",
      "\n",
      "        [[ 4.7941e-02, -6.4328e-02, -7.9503e-02]],\n",
      "\n",
      "        [[-1.2684e-01,  5.9652e-02,  6.8121e-02]],\n",
      "\n",
      "        [[-3.3316e-02,  1.7574e-01, -3.3072e-02]],\n",
      "\n",
      "        [[-1.9920e-02, -2.5308e-02, -2.2590e-02]],\n",
      "\n",
      "        [[-2.4246e-01,  1.9429e-01, -1.0980e-01]],\n",
      "\n",
      "        [[-1.1816e-01, -5.6288e-02,  9.7993e-02]],\n",
      "\n",
      "        [[-5.4766e-03,  1.2237e-01,  1.1602e-01]],\n",
      "\n",
      "        [[ 7.5381e-02,  1.7993e-01, -3.0667e-02]],\n",
      "\n",
      "        [[-1.2962e-02,  2.0181e-01,  2.8887e-03]],\n",
      "\n",
      "        [[-2.5603e-02, -2.9600e-02,  9.8122e-02]],\n",
      "\n",
      "        [[-1.1398e-02, -9.4505e-02, -1.8530e-03]],\n",
      "\n",
      "        [[ 7.9509e-02,  4.4828e-02,  3.8531e-02]],\n",
      "\n",
      "        [[ 2.9759e-02,  1.6461e-01,  9.0569e-03]],\n",
      "\n",
      "        [[-1.7462e-01,  1.4070e-01, -5.1245e-02]],\n",
      "\n",
      "        [[-3.0300e-03, -8.9320e-03,  2.4117e-01]],\n",
      "\n",
      "        [[ 1.9153e-02, -2.4430e-01,  5.1454e-03]],\n",
      "\n",
      "        [[ 1.7352e-01, -6.8888e-02, -6.9603e-02]],\n",
      "\n",
      "        [[-2.2513e-01, -1.9880e-02,  3.1605e-02]],\n",
      "\n",
      "        [[-9.0961e-02, -1.3756e-01, -1.9601e-02]],\n",
      "\n",
      "        [[-5.1966e-02,  2.4096e-02,  2.1348e-01]]], requires_grad=True) tensor([[0.1900],\n",
      "        [0.2708],\n",
      "        [0.2039],\n",
      "        [0.2066],\n",
      "        [0.1504],\n",
      "        [0.1759],\n",
      "        [0.1369],\n",
      "        [0.1710],\n",
      "        [0.1691],\n",
      "        [0.2655],\n",
      "        [0.1964],\n",
      "        [0.0395],\n",
      "        [0.3429],\n",
      "        [0.3820],\n",
      "        [0.3785],\n",
      "        [0.1355],\n",
      "        [0.1861],\n",
      "        [0.1890],\n",
      "        [0.1129],\n",
      "        [0.1558],\n",
      "        [0.1819],\n",
      "        [0.0393],\n",
      "        [0.3295],\n",
      "        [0.1635],\n",
      "        [0.1687],\n",
      "        [0.1975],\n",
      "        [0.2022],\n",
      "        [0.1056],\n",
      "        [0.0952],\n",
      "        [0.0991],\n",
      "        [0.1675],\n",
      "        [0.2300],\n",
      "        [0.2414],\n",
      "        [0.2451],\n",
      "        [0.1992],\n",
      "        [0.2282],\n",
      "        [0.1661],\n",
      "        [0.2210]], grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv1d(19, 19*2, kernel_size=3, groups=19, stride=2, padding=3, bias=False)\n",
    "print(conv_layer.weight,torch.norm(conv_layer.weight))\n",
    "nn.init.xavier_normal_(conv_layer.weight)\n",
    "print(conv_layer.weight, torch.norm(conv_layer.weight, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69183524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 1, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320dd3e",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb628e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: %0.95\n",
      "INFO:__main__:Current accuracy: %0.95\n"
     ]
    }
   ],
   "source": [
    "# Init logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "acc_example = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: %{acc_example}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535cc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/abnormal_12000.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Configs['checkpoint']['checkpoint_dir'] is not None:\n",
    "    print(Configs['checkpoint']['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3852c0",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e176c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir:str, label_dir:str, label_dict:dict, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/origin_csv/train'\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(self.label_dir)\n",
    "        self.label_dict = label_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fe17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Configs['dataset']['train_data_dir']\n",
    "train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "val_data_dir = Configs['dataset']['val_data_dir']\n",
    "val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "label_dict = Configs['dataset']['classes']\n",
    "train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                              label_dir=train_label_dir,\n",
    "                              label_dict=label_dict)\n",
    "val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                            label_dir=val_label_dir,\n",
    "                            label_dict=label_dict)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=Configs['train']['batch_size'],\n",
    "                              shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset=val_dataset, num_workers=16, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da102a99",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_22618/1770464913.py\", line 20, in __getitem__\n    label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\nKeyError: 'tuh_eeg_abnormal'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m iter_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[0;32m----> 2\u001b[0m data, target, file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miter_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_22618/1770464913.py\", line 20, in __getitem__\n    label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\nKeyError: 'tuh_eeg_abnormal'\n"
     ]
    }
   ],
   "source": [
    "iter_train = iter(train_loader)\n",
    "data, target, file = next(iter_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6de7e7",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c47be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.pe = PositionalEncoding(d_model=n_channels, max_len=input_size)\n",
    "        self.ae = resnet18(n_channels=n_channels, groups=n_channels, num_classes=classes, d_model=model_hyp['d_model'])\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.pe(x)\n",
    "        z = z.transpose(-1,-2)\n",
    "        z = self.ae(z)\n",
    "        z = self.transformer_encoder(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35257f90",
   "metadata": {},
   "source": [
    "# test model\n",
    "batch = 16\n",
    "d_model = 19\n",
    "max_len = 12000\n",
    "x = torch.rand(batch, max_len, d_model)\n",
    "pe = model(input_size=Configs['input_size'],\n",
    "           n_channels = Configs['n_channels'],\n",
    "           model_hyp=Configs['model'],\n",
    "           classes=len(Configs['dataset']['classes']))\n",
    "           \n",
    "out = pe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d756d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:models.resnet1D:data shape is: torch.Size([16, 19, 12000])\n",
      "DEBUG:models.resnet1D:data shape after conv1: torch.Size([16, 152, 6000])\n",
      "DEBUG:models.resnet1D:data shape after ln1: torch.Size([16, 152, 6000])\n",
      "DEBUG:models.resnet1D:data shape after avgpool: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:layer1\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 152, 3000]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 152, 3000]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 152, 3000]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 152, 3000]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 152, 3000])\n",
      "DEBUG:models.resnet1D:layer2\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 304, 1500]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 304, 1500]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after downsample: torch.Size([16, 304, 1500]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 304, 1500]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 304, 1500]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 304, 1500])\n",
      "DEBUG:models.resnet1D:layer3\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 608, 750]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 608, 750]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after downsample: torch.Size([16, 608, 750]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 608, 750]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 608, 750]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 608, 750])\n",
      "DEBUG:models.resnet1D:layer4\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 1216, 375]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 1216, 375]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after downsample: torch.Size([16, 1216, 375]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv1-3x3: torch.Size([16, 1216, 375]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln1: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after sub-relu1: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after sub-conv2-3x3: torch.Size([16, 1216, 375]), groups=19\n",
      "DEBUG:models.resnet1D:data shape after sub-ln2: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after residual relu+res: torch.Size([16, 1216, 375])\n",
      "DEBUG:models.resnet1D:data shape after avgpool: torch.Size([16, 19, 256])\n",
      "DEBUG:models.transformer_encoder:transformer output size: torch.Size([16, 19, 256])\n",
      "DEBUG:models.transformer_encoder:flatten output size: torch.Size([16, 4864])\n",
      "DEBUG:models.transformer_encoder:linear output size: torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Configs['dataset']['train_data_dir']\n",
    "train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "val_data_dir = Configs['dataset']['val_data_dir']\n",
    "val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "label_dict = Configs['dataset']['classes']\n",
    "train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                              label_dir=train_label_dir,\n",
    "                              label_dict=label_dict)\n",
    "val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                            label_dir=val_label_dir,\n",
    "                            label_dict=label_dict)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=Configs['train']['batch_size'],\n",
    "                              shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset=val_dataset, num_workers=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "# classifier = model(input_size=Configs['input_size'],\n",
    "#                                     n_channels = Configs['n_channels'],\n",
    "#                                     model_hyp=Configs['model'],\n",
    "#                                     classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "\n",
    "classifier = resnet18(groups=Configs['n_channels'], num_classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.999),lr=Configs['optimizer']['init_lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(Configs['tensorboard']['runs_dir']+'train_board')    # Initilize tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803bab0",
   "metadata": {},
   "source": [
    "#### Check eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5baeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_para = torch.load('weights/resnet18_best.pth')\n",
    "classifier.load_state_dict(model_para['model_state_dict'])\n",
    "classifier.cuda().eval()\n",
    "\n",
    "train_signal = iter(train_loader)\n",
    "eval_signal = iter(eval_loader)\n",
    "\n",
    "train_data = next(train_signal)\n",
    "\n",
    "x_train, y_train = train_data[0].to('cuda'), train_data[1].to('cuda')\n",
    "train_pred = classifier(x_train)\n",
    "train_loss = criterion(train_pred, y_train)\n",
    "train_loss\n",
    "\n",
    "eval_data = next(eval_signal)\n",
    "\n",
    "x_eval, y_eval = eval_data[0].to('cuda'), eval_data[1].to('cuda')\n",
    "eval_pred = classifier(x_eval)\n",
    "eval_loss = criterion(eval_pred, y_eval)\n",
    "eval_loss\n",
    "\n",
    "train_pred, eval_pred\n",
    "y_train, y_eval\n",
    "\n",
    "_, predicted = torch.max(eval_pred, 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600abd7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Validate results\n",
    "\n",
    "import torch.nn.functional as F\n",
    "val_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    outputs = classifier(data)\n",
    "    print(_)\n",
    "    loss = criterion(outputs, target)\n",
    "    val_loss += loss.item()\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    print(loss.item(), probabilities)\n",
    "    _, predicted = torch.max(probabilities, 1)\n",
    "    print(predicted, target)\n",
    "    total += target.size(0)  # Total number of samples\n",
    "    correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "    print(correct, total)\n",
    "    val_loss /= len(eval_loader)\n",
    "    val_accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39bd691",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir:str, label_dir:str, label_dict:dict, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/origin_csv/train'\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(self.label_dir)\n",
    "        self.label_dict = label_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data, label, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d0d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# label_dic = {'normal':0, 'abnormal':1}\n",
    "\n",
    "    \n",
    "# transform = transforms.Compose([\n",
    "#     transforms.MinMaxScaler(feature_range=(0, 1)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# combined_dataset = ConcatDataset([train_dataset, eval_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a157aec",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning rate update policy\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.info(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e972650",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c11398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Configs:dict):\n",
    "    train_data_dir = Configs['dataset']['train_data_dir']\n",
    "    train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "    val_data_dir = Configs['dataset']['val_data_dir']\n",
    "    val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "    label_dict = Configs['dataset']['classes']\n",
    "    train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                                  label_dir=train_label_dir,\n",
    "                                  label_dict=label_dict)\n",
    "    val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                                label_dir=val_label_dir,\n",
    "                                label_dict=label_dict)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=Configs['train']['batch_size'],\n",
    "                                  shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "    eval_loader = DataLoader(dataset=val_dataset, num_workers=16, shuffle=True, pin_memory=True)\n",
    "\n",
    "    classifier = transformer_classifier(input_size=Configs['input_size'], \n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.9),lr=Configs['optimizer']['init_lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(Configs['tensorboard']['runs_dir']+'train_board')    # Initilize tensorflow\n",
    "    \n",
    "    ### Warmup training\n",
    "    warmup_steps = Configs['train']['warmup_steps']\n",
    "    warmup_step = 0\n",
    "    min_loss = 1\n",
    "    while warmup_step < warmup_steps:\n",
    "        for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "            if warmup_step < warmup_steps:\n",
    "        #     for batch_index, data in enumerate(train_loader, 0):\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "                y = classifier(data)\n",
    "        #         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "                warmup_loss = criterion(y, target)\n",
    "                optimizer.zero_grad()\n",
    "                warmup_loss.backward()\n",
    "                optimizer.step()\n",
    "        #         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "                logger.info(f\"Warmup Step: {warmup_step}, Warmup Loss: {warmup_loss}\")\n",
    "                writer.add_scalar('Warmup Loss', warmup_loss, global_step=warmup_step)\n",
    "                warmup_step += 1\n",
    "        \n",
    "#         if warmup_step%5==0:\n",
    "#             torch.save(classifier.state_dict(), \n",
    "#                        Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_last.pth')\n",
    "        if warmup_loss < min_loss:\n",
    "            torch.save(classifier.state_dict(),\n",
    "                       Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_best.pth')\n",
    "            min_loss = warmup_loss\n",
    "        \n",
    "    \n",
    "    ### train\n",
    "#     step = 0\n",
    "#     epochs = Configs['train']['n_epochs']\n",
    "#     for epoch in range(epochs):\n",
    "#         # Training loop\n",
    "#         poly_lr_scheduler(optimizer, init_lr=Configs['optimizer']['init_lr'], iter=epoch, max_iter=epochs)\n",
    "#         for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "#             data, target = data.to('cuda'), target.to('cuda')\n",
    "#             y = classifier(data)\n",
    "#     #         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "#             train_loss = criterion(y, target)\n",
    "#             optimizer.zero_grad()\n",
    "#             train_loss.backward()\n",
    "#             optimizer.step()\n",
    "#     #         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "#             logger.info(f\"Step: {step}, training Loss: {train_loss}\")\n",
    "#             writer.add_scalar('Training Loss', train_loss, global_step=step)\n",
    "#             step += 1\n",
    "        \n",
    "#         if epoch%5==0:\n",
    "#             val_loss = 0\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             accuracy = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "#                     data, target = data.to('cuda'), target.to('cuda')\n",
    "#                     outputs = classifier(data)\n",
    "#                     loss = criterion(outputs, target)\n",
    "#                     val_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs, 1)\n",
    "#                     total += target.size(0)  # Total number of samples\n",
    "#                     correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "\n",
    "#             val_loss /= len(eval_loader)\n",
    "#             accuracy = 100 * correct / total\n",
    "#             writer.add_scalar('Validation Loss', val_loss, global_step=step)\n",
    "#             writer.add_scalar('Validation Accuracy', accuracy, global_step=step)\n",
    "#             logger.info(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "#         torch.save(classifier.state_dict(), \n",
    "#                    Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_latest.pth')\n",
    "#         if train_loss < min_loss:\n",
    "#             torch.save(classifier.state_dict(),\n",
    "#                        Configs['checkpoint']['checkpoint_dir']+'inte_transformer_params_best.pth')\n",
    "#             min_loss = train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4333cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/eeg_torch.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    configs = yaml.safe_load(file)\n",
    "    \n",
    "# configs['optimizer']['init_lr']\n",
    "# train(Configs=configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a30a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = 19\n",
    "ae_1 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                               stride=3, kernel_size=7, dilation=1, groups=n_channels,\n",
    "                               padding_mode='reflect'),\n",
    "                     nn.BatchNorm1d(n_channels),\n",
    "                     nn.ReLU())\n",
    "ae_2 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                               stride=2, kernel_size=4, dilation=1, groups=n_channels,\n",
    "                               padding_mode='reflect'),\n",
    "                     nn.BatchNorm1d(n_channels),\n",
    "                     nn.ReLU())\n",
    "ae_3 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                               stride=2, kernel_size=4, dilation=1, groups=n_channels,\n",
    "                               padding_mode='reflect'),\n",
    "                     nn.BatchNorm1d(n_channels),\n",
    "                     nn.ReLU())\n",
    "\n",
    "# max_pool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "in_data = torch.randn(20, 19, 6000)\n",
    "out_1 = ae_1(in_data)\n",
    "out_2 = ae_2(out_1)\n",
    "out_3 = ae_3(out_2)\n",
    "# out_data = max_pool(out_data)\n",
    "out_1.shape, out_2.shape, out_3.shape\n",
    "# 5660 stride 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40261902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_classifier(nn.Module):\n",
    "    def __init__(self, input_size:int, n_channels:int, model_hyp:dict, classes:int):\n",
    "        super(transformer_classifier, self).__init__()\n",
    "        \n",
    "        self.ae_1 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                                     stride=3, kernel_size=7, dilation=1, groups=n_channels,\n",
    "                                            padding_mode='reflect'),\n",
    "                                 nn.BatchNorm1d(n_channels),\n",
    "                                 nn.ReLU())\n",
    "        \n",
    "        self.ae_2 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                                     stride=2, kernel_size=4, dilation=1, groups=n_channels,\n",
    "                                            padding_mode='reflect'),\n",
    "                                 nn.BatchNorm1d(n_channels),\n",
    "                                 nn.ReLU())\n",
    "\n",
    "        self.ae_3 = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=n_channels, \n",
    "                                     stride=2, kernel_size=4, dilation=1, groups=n_channels,\n",
    "                                            padding_mode='reflect'),\n",
    "                                 nn.BatchNorm1d(n_channels),\n",
    "                                 nn.ReLU())\n",
    "#         self.hidden_size = 597    # need to be calculated every time if you change shape of input\n",
    "#         self.ae = AutoEncoder(input_size=input_size, hidden_size=model_hyp['d_model'])  \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_hyp['d_model'],\n",
    "                                                        nhead=model_hyp['n_head'])\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=model_hyp['n_layer'])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(model_hyp['d_model']*n_channels, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.ae_1(x)\n",
    "        z = self.ae_2(z)\n",
    "        z = self.ae_3(z)\n",
    "#         z = z[:, :, :1496] \n",
    "        logger.debug(f\"ae output size: %{z.shape}\")\n",
    "        z = self.transformer_encoder(z)\n",
    "        logger.debug(f\"transformer output size: %{z.shape}\")\n",
    "        z = self.flatten(z)\n",
    "        logger.debug(f\"flatten output size: %{z.shape}\")\n",
    "        y = self.linear(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8046f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = torch.randn(20, 19, 6000).to('cuda')\n",
    "classifier = transformer_classifier(input_size=6000, n_channels =19,model_hyp=configs['model'],classes=len(configs['dataset']['classes'])).to('cuda')\n",
    "out = classifier(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592583d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target output size of 5\n",
    "m = nn.AdaptiveAvgPool2d((19,5))\n",
    "input = torch.randn(1, 64, 8)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = resnet18()\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.ae(x)\n",
    "        z = self.transformer_encoder(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8e1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "#     logging.basicConfig(filename=f'./logs/{datetime.now().strftime(\"%y%m%d%H%M\")}_resnet18.log',level=logging.DEBUG)\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    # logger.setLevel(logging.DEBUG)\n",
    "    handler = logging.StreamHandler()\n",
    "    # formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    # handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    resnet18 = resnet18(groups=Configs['n_channels'], num_classes=len(Configs['dataset']['classes']), \n",
    "                        d_model=Configs['model']['d_model'])\n",
    "    in_data = torch.randn(20, 19, 6000)\n",
    "    out = resnet18(in_data)\n",
    "    out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python38]",
   "language": "python",
   "name": "conda-env-python38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
