{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import argparse\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from configs.config import configs\n",
    "\n",
    "# from models.pe import PositionalEncoding\n",
    "from models.FusionCNN import EEG_CNN, FusionNetwork, EEG_Pathology_Detection\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "\n",
    "\n",
    "# Transform signal\n",
    "def transform(data:Tensor, mean:Tensor, std:Tensor):\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "# ### Dataset\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir:str, label_dir:str, label_dict:dict, mean: list, std: list, transform=None):\n",
    "        \n",
    "        self.data_dir = data_dir   # './data/seg_csv/train'\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(self.label_dir)\n",
    "        self.label_dict = label_dict\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "#         label = torch.tensor(int(self.label_dict[self.annotations.iloc[index,1]]))\n",
    "        label = self.annotations.loc[self.annotations['csv_file'] == file_name, ['label']].to_string(index=False,header=False)\n",
    "        label = torch.tensor(int(self.label_dict[label]))\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data, self.mean, self.std)\n",
    "            \n",
    "        return (data.t(), label, file_name)\n",
    "\n",
    "\n",
    "# class model(nn.Module):\n",
    "#     def __init__(self, input_size: int, input_channels: int, model_hyp: dict, classes: int):\n",
    "#         super(model, self).__init__()\n",
    "#         self.cnn_model = EEG_CNN(input_channels=19, classes=2)\n",
    "#         self.fusion_model = FusionNetwork(input_sizes=[50 * 3997, 100 * 1329, 100 * 440, 200 * 143], fusion_type=\"MLP\", hidden_layers=[8192, 4096], classes=2)\n",
    "#         self.model = EEG_Pathology_Detection(self.cnn_model, self.fusion_model)\n",
    "        \n",
    "#         self.reset_parameters()\n",
    "        \n",
    "#     def reset_parameters(self):\n",
    "#         r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "#         for p in self.parameters():\n",
    "#             if p.dim() > 1:\n",
    "# #                 logger.debug(p.shape)\n",
    "#                 nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "#         for m in self.modules():\n",
    "# #             print(m)\n",
    "#             if isinstance(m, nn.Conv1d):\n",
    "#                 nn.init.xavier_uniform_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "        \n",
    "#             elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "#                 nn.init.ones_(m.weight)\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "#         print('Complete initiate parameters')\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         y = self.model(x)\n",
    "        \n",
    "#         return y\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "### Learning rate update policy\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    curr_lr_rt = optimizer.param_groups[0]['lr']\n",
    "    logger.info(f'lr=: {curr_lr_rt}')\n",
    "    return curr_lr_rt\n",
    "\n",
    "    # Predict a single signal\n",
    "    def predict_signal(model, signal):\n",
    "        with torch.no_grad():\n",
    "            output = model(signal)\n",
    "            # For EEGNet\n",
    "    #         output = model(signal.unsqueeze(1)).to('cpu')\n",
    "\n",
    "            prediction = torch.argmax(output, dim=1).item()  # Get the predicted class\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    signal_to_case_map = []  # Map each signal index to a case ID, e.g., [case1, case1, case2, ...]\n",
    "    for data, label, file_name in dataloader:\n",
    "        signal_to_case_map.append(file_name[0].split('_')[0])\n",
    "    \n",
    "    # Predict a single signal\n",
    "    def predict_signal(model, signal):\n",
    "        with torch.no_grad():\n",
    "            output = model(signal)\n",
    "            # For NNGNet\n",
    "    #         output = model(signal.unsqueeze(1)).to('cpu')\n",
    "        \n",
    "        prediction = torch.argmax(output, dim=1).item()  # Get the predicted class\n",
    "        return prediction\n",
    "\n",
    "    # Calculate sensitivity, specificity, and accuracy\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.tolist()\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.tolist()\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        return sensitivity, specificity, accuracy\n",
    "\n",
    "    # Evaluate per-signal metrics\n",
    "    def evaluate_per_signal(model, dataset):\n",
    "        y_true, y_pred = [], []\n",
    "        for signal, label, _ in dataset:\n",
    "            signal= signal.to('cuda')\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.item()\n",
    "            prediction = predict_signal(model, signal)\n",
    "            y_true.append(label)\n",
    "            y_pred.append(prediction)\n",
    "        sensitivity, specificity, accuracy = calculate_metrics(y_true, y_pred)\n",
    "        return sensitivity, specificity, accuracy, y_true, y_pred\n",
    "\n",
    "    # Aggregate signals for per-case metrics\n",
    "    def evaluate_per_case(y_true, y_pred, signal_to_case_map):\n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.tolist()\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.tolist()\n",
    "\n",
    "        case_results = {}\n",
    "        for signal_idx, case_id in enumerate(signal_to_case_map):\n",
    "            if case_id not in case_results:\n",
    "                case_results[case_id] = {'true': [], 'pred': []}\n",
    "            case_results[case_id]['true'].append(y_true[signal_idx])\n",
    "            case_results[case_id]['pred'].append(y_pred[signal_idx])\n",
    "\n",
    "        # Per-case metrics\n",
    "        y_true_case, y_pred_case = [], []\n",
    "        for case_id, results in case_results.items():\n",
    "            # Majority vote for case prediction\n",
    "            true_label = max(set(results['true']), key=results['true'].count)\n",
    "            pred_label = max(set(results['pred']), key=results['pred'].count)\n",
    "            y_true_case.append(true_label)\n",
    "            y_pred_case.append(pred_label)\n",
    "\n",
    "        sensitivity, specificity, accuracy = calculate_metrics(y_true_case, y_pred_case)\n",
    "        return sensitivity, specificity, accuracy, y_true_case, y_pred_case\n",
    "\n",
    "    # Per-signal evaluation\n",
    "    logger.info(\"Evaluating per-signal metrics...\")\n",
    "    sensitivity_signal, specificity_signal, accuracy_signal, y_true_signal, y_pred_signal = evaluate_per_signal(\n",
    "        model, dataloader)\n",
    "    logger.info(f\"Per-Signal Sensitivity: {sensitivity_signal:.4f}, Specificity: {specificity_signal:.4f}, Accuracy {accuracy_signal:.4f}\")\n",
    "\n",
    "    # Per-case evaluation\n",
    "    logger.info(\"Evaluating per-case metrics...\")\n",
    "    sensitivity_case, specificity_case, accuracy_case, y_true_case, y_pred_case = evaluate_per_case(\n",
    "        y_true_signal, y_pred_signal, signal_to_case_map)\n",
    "    logger.info(f\"Per-Case Sensitivity: {sensitivity_case:.4f}, Specificity: {specificity_case:.4f}, Accuracy: {accuracy_case:.4f}\")\n",
    "    return sensitivity_signal, specificity_signal, accuracy_signal, sensitivity_case, specificity_case, accuracy_case\n",
    "\n",
    "def freeze_layers(model, freeze=True):\n",
    "    \"\"\"\n",
    "    Freezes or unfreezes all parameters in a given model.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = not freeze    \n",
    "    \n",
    "\n",
    "def train(Configs:dict):\n",
    "    train_data_dir = Configs['dataset']['train_data_dir']\n",
    "    train_label_dir = Configs['dataset']['train_label_dir']\n",
    "\n",
    "    val_data_dir = Configs['dataset']['val_data_dir']\n",
    "    val_label_dir = Configs['dataset']['val_label_dir']\n",
    "\n",
    "    label_dict = Configs['dataset']['classes']\n",
    "    \n",
    "    mean = Configs['dataset']['mean']\n",
    "    std = Configs['dataset']['std']\n",
    "    \n",
    "    model_name = Configs['model']['name']\n",
    "    \n",
    "    train_dataset = customDataset(data_dir=train_data_dir,\n",
    "                                  label_dir=train_label_dir,\n",
    "                                  label_dict=label_dict,\n",
    "                                 mean=mean, std=std,\n",
    "                                 transform=transform)\n",
    "    val_dataset = customDataset(data_dir=val_data_dir,\n",
    "                                label_dir=val_label_dir,\n",
    "                                label_dict=label_dict,\n",
    "                               mean=mean, std=std,\n",
    "                               transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=Configs['train']['batch_size'],\n",
    "                              shuffle=Configs['dataset']['shuffle'], \n",
    "                              num_workers=Configs['dataset']['num_workers'], pin_memory=True)\n",
    "\n",
    "    eval_loader = DataLoader(dataset=val_dataset, num_workers=Configs['dataset']['num_workers'], \n",
    "                             shuffle=Configs['dataset']['shuffle'], pin_memory=True)\n",
    "    \n",
    "\n",
    "#     input_layer = nn.Sequential(\n",
    "# #         nn.Embedding(num_embeddings=10000, embedding_dim=512),\n",
    "# #         PositionalEncoding(d_model=512, dropout=0.1, max_len=5000)\n",
    "#         PositionalEncoding(d_model=Configs['n_channels'], max_len=Configs['input_size'])\n",
    "#     ).to('cuda')\n",
    "\n",
    "    # Training Stage 1\n",
    "    if Configs['checkpoint']['weights'] is not None:\n",
    "        print(f'loading pre-trained model...')\n",
    "        cnn_model = torch.load(Configs['checkpoint']['checkpoint_dir']+Configs['checkpoint']['weights'])\n",
    "#         classifier.load_state_dict(state_dict)\n",
    "    else:\n",
    "        cnn_model = EEG_CNN(input_channels=Configs['n_channels'],\n",
    "                            classes=len(Configs['dataset']['classes']), stage=1).to('cuda')\n",
    "        freeze_layers(cnn_model, freeze=False)\n",
    "        optimizer = torch.optim.Adam(cnn_model.parameters(), lr=Configs['optimizer']['init_lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "#         classifier = model(input_size=Configs['input_size'],\n",
    "#                                         input_channels = Configs['n_channels'],\n",
    "#                                         model_hyp=Configs['model'],\n",
    "#                                         classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(classifier.parameters(),lr=Configs['optimizer']['init_lr'], weight_decay=Configs['optimizer']['weight_decay'])\n",
    "#     criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n",
    "    writer = SummaryWriter(Configs['tensorboard']['runs_dir']+f'{datetime.now().strftime(\"%y%m%d%H%M\")}_{model_name}_train_board')   # Initilize tensorboard\n",
    "    \n",
    "    min_loss = 0.7\n",
    "    best_accuracy = 0.65\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if Configs['warmup']==1:\n",
    "        ### Warmup training\n",
    "        warmup_steps = Configs['train']['warmup_steps']\n",
    "        warmup_step = 0\n",
    "\n",
    "        while warmup_step < warmup_steps:\n",
    "            \n",
    "            for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "                cnn_model.train()\n",
    "                if warmup_step < warmup_steps:\n",
    "                    optimizer.zero_grad()\n",
    "            #     for batch_index, data in enumerate(train_loader, 0):\n",
    "                    data, target = data.to('cuda'), target.to('cuda')\n",
    "#                     input_pe = input_layer(data)\n",
    "                    y = cnn_model(data)\n",
    "            #         logger.debug(f\"y size:{y.shape}, target size{target.shape}\")\n",
    "                    warmup_loss = criterion(y, target)\n",
    "                    \n",
    "                    warmup_loss.backward()\n",
    "                    optimizer.step()\n",
    "            #         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "                    logger.info(f\"Warmup Step: {warmup_step}, Warmup Loss: {warmup_loss}\")\n",
    "                    writer.add_scalar('Warmup Loss', warmup_loss, global_step=warmup_step)\n",
    "#                     writer.flush()\n",
    "                    warmup_step += 1\n",
    "\n",
    "                if warmup_loss < min_loss:  # evaluate model\n",
    "                    min_loss = warmup_loss\n",
    "    \n",
    "    \n",
    "    #Start training-stage 1\n",
    "    ## load pre-trained model and train\n",
    "    step = 0\n",
    "    epochs = Configs['train']['n_epochs']\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        curr_lr_rt = poly_lr_scheduler(optimizer, init_lr=Configs['optimizer']['init_lr'], iter=epoch, max_iter=epochs)\n",
    "        for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            data, target = data.to('cuda'), target.to('cuda')\n",
    "            y = cnn_model(data)\n",
    "            train_loss = criterion(y, target)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            logger.info(f\"Epoch: {epoch+1}, Step: {step}, training Loss: {train_loss}\")\n",
    "            writer.add_scalar('Training Loss', train_loss, global_step=step)\n",
    "#             writer.flush()\n",
    "            step += 1\n",
    "\n",
    "        sensitivity_signal, specificity_signal, accuracy_signal, sensitivity_case, specificity_case, accuracy_case = evaluate_model(cnn_model, eval_loader, criterion)\n",
    "#         writer.add_scalar('Validation Loss', val_loss, global_step=step)\n",
    "#         writer.add_scalar('Validation Accuracy', val_accuracy, global_step=step)\n",
    "#         writer.add_scalar('Validation Accuracy with epoch', val_accuracy, global_step=epoch)\n",
    "        \n",
    "#         if val_accuracy > best_accuracy:\n",
    "#             best_accuracy = val_accuracy\n",
    "        torch.save(cnn_model, Configs['checkpoint']['checkpoint_dir']+ f'{datetime.now().strftime(\"%y%m%d%H%M\")}_{model_name}_params_epoch_{epoch+1}_acc{accuracy_signal:04f}.pth')\n",
    "        writer.add_hparams({'lr': curr_lr_rt, 'bsize': Configs['train']['batch_size'], 'input_size': Configs['input_size'], 'epoch': epoch+1},{'sensitivity_signal': sensitivity_signal*100, 'specificity_signal': specificity_signal*100, 'accuracy_signal': accuracy_signal*100, 'sensitivity_case': sensitivity_case*100, 'specificity_case': specificity_case*100, 'accuracy_case': accuracy_case*100})\n",
    "#             writer.flush()\n",
    "            \n",
    "            \n",
    "           \n",
    "    \n",
    "    \n",
    "    # train stage 2\n",
    "    cnn_model.stage = 2\n",
    "    cnn_model.fc1 = None\n",
    "    cnn_model.fc2 = None\n",
    "    cnn_model.softmax = None\n",
    "    \n",
    "    fusion_model = FusionNetwork(\n",
    "        input_sizes=[50 * 3997, 100 * 1329, 100 * 440, 200 * 143],\n",
    "        fusion_type=\"MLP\",\n",
    "        hidden_layers=[8192, 4096],\n",
    "        classes=len(Configs['dataset']['classes'])).to('cuda')\n",
    "    \n",
    "    combined_model = EEG_Pathology_Detection(cnn_model, fusion_model).to('cuda')\n",
    "    \n",
    "    freeze_layers(cnn_model, freeze=True)\n",
    "    freeze_layers(fusion_model, freeze=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, combined_model.parameters()), lr=Configs['optimizer']['init_lr'])\n",
    "    \n",
    "    step = 0\n",
    "    epochs = Configs['train']['n_epochs']\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        curr_lr_rt = poly_lr_scheduler(optimizer, init_lr=Configs['optimizer']['init_lr'], iter=epoch, max_iter=epochs)\n",
    "        for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            data, target = data.to('cuda'), target.to('cuda')\n",
    "            y = combined_model(data)\n",
    "            train_loss = criterion(y, target)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            logger.info(f\"Epoch: {epoch+1}, Step: {step}, training Loss: {train_loss}\")\n",
    "            writer.add_scalar('Training Loss', train_loss, global_step=step)\n",
    "#             writer.flush()\n",
    "            step += 1\n",
    "\n",
    "        sensitivity_signal, specificity_signal, accuracy_signal, sensitivity_case, specificity_case, accuracy_case = evaluate_model(cnn_model, eval_loader, criterion)\n",
    "#         writer.add_scalar('Validation Loss', val_loss, global_step=step)\n",
    "#         writer.add_scalar('Validation Accuracy', val_accuracy, global_step=step)\n",
    "#         writer.add_scalar('Validation Accuracy with epoch', val_accuracy, global_step=epoch)\n",
    "        \n",
    "#         if val_accuracy > best_accuracy:\n",
    "#             best_accuracy = val_accuracy\n",
    "        torch.save(cnn_model, Configs['checkpoint']['checkpoint_dir']+ f'{datetime.now().strftime(\"%y%m%d%H%M\")}_{model_name}_params_epoch_{epoch+1}_acc{accuracy_signal:04f}.pth')\n",
    "        writer.add_hparams({'lr': curr_lr_rt, 'bsize': Configs['train']['batch_size'], 'input_size': Configs['input_size'], 'epoch': epoch+1},{'sensitivity_signal': sensitivity_signal*100, 'specificity_signal': specificity_signal*100, 'accuracy_signal': accuracy_signal*100, 'sensitivity_case': sensitivity_case*100, 'specificity_case': specificity_case*100, 'accuracy_case': accuracy_case*100})\n",
    "#             writer.flush()\n",
    "    \n",
    "    \n",
    "    \n",
    "    writer.close()\n",
    "    end_time = time.time() \n",
    "    # Convert elapsed time to hours, minutes, and seconds\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours = int(elapsed_time // 3600)\n",
    "    minutes = int((elapsed_time % 3600) // 60)\n",
    "    seconds = int(elapsed_time % 60)\n",
    "    # Format the time as \"xh ym zs\"\n",
    "#     formatted_time = f\"{hours}h {minutes}m {seconds}s\"\n",
    "    logger.info(f\"Training time: {hours}h {minutes}m {seconds}s\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "    # parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "    # parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "    args = parser.parse_args(args=['configs/FusionCNN.yml'])\n",
    "    \n",
    "    with open(args.config_file, 'r') as file:\n",
    "        configs = yaml.safe_load(file)\n",
    "    model_name = configs['model']['name']\n",
    "    logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "#     logger.setLevel(logging.DEBUG)\n",
    "    handler = logging.StreamHandler()\n",
    "    # formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    # handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    train(Configs=configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94f00d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnn_model = EEG_CNN(input_channels=19, classes=2)\n",
    "# fusion_model = FusionNetwork(input_sizes=[50 * 3997, 100 * 1329, 100 * 440, 200 * 143], fusion_type=\"MLP\", hidden_layers=[8192, 4096], classes=2)\n",
    "# model = EEG_Pathology_Detection(cnn_model, fusion_model)\n",
    "# x = torch.randn(1,19,12000)\n",
    "# # debug\n",
    "\n",
    "# # Forward pass\n",
    "# pool1, pool2, pool3, pool4 = cnn_model(x)\n",
    "\n",
    "\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(x)\n",
    "# flops = FlopCountAnalysis(model, x)\n",
    "# print(\"FLOPs:\", flops.total())\n",
    "# print(parameter_count_table(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
