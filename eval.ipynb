{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974294bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import logging\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from models.encoder import res_encoderS\n",
    "from models.EEGNET import EEGNet\n",
    "\n",
    "from models.classifier import transformer_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee844305",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderS+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6335c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform signal\n",
    "def transform(data:Tensor, mean:Tensor, std:Tensor):\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "# ### Dataset\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir:str, label_dir:str, label_dict:dict, mean: list, std: list, transform=None):\n",
    "        \n",
    "        self.data_dir = data_dir   # './data/seg_csv/train'\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(self.label_dir)\n",
    "        self.label_dict = label_dict\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = self.annotations.loc[self.annotations['csv_file'] == file_name, ['label']].to_string(index=False,header=False)\n",
    "        label = torch.tensor(int(self.label_dict[label]))\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data, self.mean, self.std)\n",
    "            \n",
    "        return (data, label, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be741f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class model(nn.Module):\n",
    "        def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "            super(model, self).__init__()\n",
    "            self.ae = res_encoderS(n_channels=n_channels, groups=n_channels, num_classes=classes, d_model=model_hyp['d_model'])\n",
    "            self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "        def reset_parameters(self):\n",
    "            r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "\n",
    "            for p in self.parameters():\n",
    "                if p.dim() > 1:\n",
    "    #                 logger.debug(p.shape)\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "            for m in self.modules():\n",
    "    #             print(m)\n",
    "                if isinstance(m, nn.Conv1d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "                elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "            print('Complete initiate parameters')\n",
    "\n",
    "        def forward(self, x):\n",
    "            z = x.transpose(-1,-2)\n",
    "            z = self.ae(z)\n",
    "            y = self.transformer_encoder(z)\n",
    "            return y\n",
    "\n",
    "\n",
    "# Load your trained model\n",
    "def load_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    model = model.to('cuda')\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a single signal\n",
    "def predict_signal(model, signal):\n",
    "    with torch.no_grad():\n",
    "        output = model(signal)\n",
    "        \n",
    "        prediction = torch.argmax(output, dim=1).item()  # Get the predicted class\n",
    "    return prediction\n",
    "\n",
    "# Calculate sensitivity, specificity, and accuracy\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.tolist()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.tolist()\n",
    "        \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return sensitivity, specificity, accuracy\n",
    "\n",
    "# Evaluate per-signal metrics\n",
    "def evaluate_per_signal(model, dataset):\n",
    "    y_true, y_pred = [], []\n",
    "    for signal, label, _ in dataset:\n",
    "        signal= signal.to('cuda')\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        prediction = predict_signal(model, signal)\n",
    "        y_true.append(label)\n",
    "        y_pred.append(prediction)\n",
    "    sensitivity, specificity, accuracy = calculate_metrics(y_true, y_pred)\n",
    "    return sensitivity, specificity, accuracy, y_true, y_pred\n",
    "\n",
    "# Aggregate signals for per-case metrics\n",
    "def evaluate_per_case(y_true, y_pred, signal_to_case_map):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.tolist()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.tolist()\n",
    "        \n",
    "    case_results = {}\n",
    "    for signal_idx, case_id in enumerate(signal_to_case_map):\n",
    "        if case_id not in case_results:\n",
    "            case_results[case_id] = {'true': [], 'pred': []}\n",
    "        case_results[case_id]['true'].append(y_true[signal_idx])\n",
    "        case_results[case_id]['pred'].append(y_pred[signal_idx])\n",
    "\n",
    "    # Per-case metrics\n",
    "    y_true_case, y_pred_case = [], []\n",
    "    for case_id, results in case_results.items():\n",
    "        # Majority vote for case prediction\n",
    "        true_label = max(set(results['true']), key=results['true'].count)\n",
    "        pred_label = max(set(results['pred']), key=results['pred'].count)\n",
    "        y_true_case.append(true_label)\n",
    "        y_pred_case.append(pred_label)\n",
    "\n",
    "    sensitivity, specificity, accuracy = calculate_metrics(y_true_case, y_pred_case)\n",
    "    return sensitivity, specificity, accuracy, y_true_case, y_pred_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "def main():\n",
    "    eval_data_dir = Configs['dataset']['val_data_dir']\n",
    "    eval_label_dir = Configs['dataset']['val_label_dir']\n",
    "    label_dict = Configs['dataset']['classes']\n",
    "    mean = Configs['dataset']['mean']\n",
    "    std = Configs['dataset']['std']\n",
    "    eval_dataset = customDataset(data_dir=eval_data_dir,\n",
    "                                label_dir=eval_label_dir,\n",
    "                                label_dict=label_dict,\n",
    "                               mean=mean, std=std,\n",
    "                               transform=transform)\n",
    "\n",
    "    dataloader = DataLoader(dataset=eval_dataset, num_workers=Configs['dataset']['num_workers'], \n",
    "                             shuffle=False, pin_memory=True)  # Replace with your dataset loader, e.g., [(signal, label), ...]\n",
    "    \n",
    "    \n",
    "    signal_to_case_map = []  # Map each signal index to a case ID, e.g., [case1, case1, case2, ...]\n",
    "    for data, label, file_name in dataloader:\n",
    "        signal_to_case_map.append(file_name[0].split('_')[0])\n",
    "\n",
    "    # Load the trained model\n",
    "    model_path = Configs['checkpoint']['checkpoint_dir']+Configs['checkpoint']['weights']\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Per-signal evaluation\n",
    "    print(\"Evaluating per-signal metrics...\")\n",
    "    sensitivity_signal, specificity_signal, accuracy_signal, y_true_signal, y_pred_signal = evaluate_per_signal(\n",
    "        model, dataloader)\n",
    "    print(f\"Per-Signal Sensitivity: {sensitivity_signal:.4f}, Specificity: {specificity_signal:.4f}, Accuracy: {accuracy_signal:.4f}\")\n",
    "\n",
    "    # Per-case evaluation\n",
    "    print(\"Evaluating per-case metrics...\")\n",
    "    sensitivity_case, specificity_case, accuracy_case, y_true_case, y_pred_case = evaluate_per_case(\n",
    "        y_true_signal, y_pred_signal, signal_to_case_map)\n",
    "    print(f\"Per-Case Sensitivity: {sensitivity_case:.4f}, Specificity: {specificity_case:.4f}, Accuracy: {accuracy_case:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088efe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "863px",
    "right": "20px",
    "top": "125px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
