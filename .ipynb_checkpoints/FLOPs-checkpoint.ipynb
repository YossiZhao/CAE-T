{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bee52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 15:39:17.802512: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-26 15:39:17.821695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-26 15:39:18.096064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "%matplotlib notebook\n",
    "\n",
    "from models.encoder import res_encoderS, res_encoderM\n",
    "from models.classifier import transformer_classifier\n",
    "from models.EEGNET import EEGNet\n",
    "\n",
    "num_train = 2717\n",
    "num_eval = 276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f280471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: %0.95\n",
      "INFO:__main__:Current accuracy: %0.95\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "acc_example = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: %{acc_example}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6838e",
   "metadata": {},
   "source": [
    "### FLOPs for auto-encoderS + transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebd8dd19",
   "metadata": {},
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderS+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1037171",
   "metadata": {},
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = res_encoderS(n_channels=n_channels, groups=n_channels, num_classes=classes, \n",
    "                               len_feature=input_size, d_model=model_hyp['d_model'])\n",
    "#         self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "#         z = x.transpose(-1,-2)\n",
    "        z = self.ae(x)\n",
    "#         z = torch.flatten(z, 1)\n",
    "#         y = self.mlp(z)\n",
    "        y = self.transformer_encoder(z)\n",
    "        return y\n",
    "        \n",
    "classifier_s = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes']))\n",
    "classifier_s.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ff03144",
   "metadata": {},
   "source": [
    "data_s = torch.randn(1, 19, 12000)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23d63069",
   "metadata": {},
   "source": [
    "# out = classifier(data_s)\n",
    "# # loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_s, data_s)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f322f9",
   "metadata": {},
   "source": [
    "### FLOPs for auto-encoderM + transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/encoderM+transformer.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    Configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb210d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initiate parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yossi/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (ae): AutoEncoder(\n",
       "    (conv1): Conv1d(19, 76, kernel_size=(64,), stride=(2,), padding=(3,), groups=19, bias=False)\n",
       "    (avgpool1d): AdaptiveAvgPool1d(output_size=12000)\n",
       "    (ln1): LayerNorm((12000,), eps=1e-05, elementwise_affine=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (avgpool_1): AvgPool1d(kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=6000)\n",
       "          (ln1): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=6000)\n",
       "          (ln2): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=6000)\n",
       "          (ln1): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(76, 76, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=6000)\n",
       "          (ln2): LayerNorm((6000,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(76, 152, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(152, 152, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln2): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(76, 152, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(152, 152, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln1): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(152, 152, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=3000)\n",
       "          (ln2): LayerNorm((3000,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(152, 304, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(304, 304, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln2): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(152, 304, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(304, 304, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln1): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(304, 304, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=1500)\n",
       "          (ln2): LayerNorm((1500,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(304, 608, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(608, 608, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln2): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(304, 608, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(608, 608, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln1): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(608, 608, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=750)\n",
       "          (ln2): LayerNorm((750,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv1d(608, 1216, kernel_size=(16,), stride=(2,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(1216, 1216, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln2): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv1d(608, 1216, kernel_size=(1,), stride=(2,), groups=19, bias=False)\n",
       "            (1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv1d(1216, 1216, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_1): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln1): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv1d(1216, 1216, kernel_size=(16,), stride=(1,), padding=(1,), groups=19, bias=False)\n",
       "          (avgpool_2): AdaptiveAvgPool1d(output_size=375)\n",
       "          (ln2): LayerNorm((375,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool_2): AdaptiveAvgPool2d(output_size=(19, 256))\n",
       "    (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    (dropout5): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): transformer_classifier(\n",
       "    (encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (linear): Linear(in_features=4864, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, input_size: int, n_channels: int, model_hyp: dict, classes: int):\n",
    "        super(model, self).__init__()\n",
    "        self.ae = res_encoderM(n_channels=n_channels, groups=n_channels, num_classes=classes, \n",
    "                               len_feature=input_size, d_model=model_hyp['d_model'])\n",
    "#         self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        self.transformer_encoder = transformer_classifier(input_size, n_channels, model_hyp, classes)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the model.\"\"\"\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "#                 logger.debug(p.shape)\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                    \n",
    "        for m in self.modules():\n",
    "#             print(m)\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        \n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "        print('Complete initiate parameters')\n",
    "\n",
    "    def forward(self, x):\n",
    "#         z = self.pe(x)\n",
    "#         z = x.transpose(-1,-2)\n",
    "        z = self.ae(x)\n",
    "#         z = torch.flatten(z, 1)\n",
    "#         y = self.mlp(z)\n",
    "        y = self.transformer_encoder(z)\n",
    "        return y\n",
    "        \n",
    "classifier_m = model(input_size=Configs['input_size'],\n",
    "                                        n_channels = Configs['n_channels'],\n",
    "                                        model_hyp=Configs['model'],\n",
    "                                        classes=len(Configs['dataset']['classes']))\n",
    "classifier_m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53aa211c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::adaptive_avg_pool1d encountered 21 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::avg_pool1d encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 10 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 4 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 1 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 2 time(s)\n",
      "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "ae.dropout5, transformer_encoder.encoder_layer, transformer_encoder.encoder_layer.dropout, transformer_encoder.encoder_layer.dropout1, transformer_encoder.encoder_layer.dropout2, transformer_encoder.encoder_layer.linear1, transformer_encoder.encoder_layer.linear2, transformer_encoder.encoder_layer.norm1, transformer_encoder.encoder_layer.norm2, transformer_encoder.encoder_layer.self_attn, transformer_encoder.encoder_layer.self_attn.out_proj, transformer_encoder.transformer_encoder.layers.0.self_attn.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 3281635008\n",
      "| name                                              | #elements or shape   |\n",
      "|:--------------------------------------------------|:---------------------|\n",
      "| model                                             | 8.6M                 |\n",
      "|  ae                                               |  6.0M                |\n",
      "|   ae.conv1                                        |   4.9K               |\n",
      "|    ae.conv1.weight                                |    (76, 1, 64)       |\n",
      "|   ae.ln1                                          |   24.0K              |\n",
      "|    ae.ln1.weight                                  |    (12000,)          |\n",
      "|    ae.ln1.bias                                    |    (12000,)          |\n",
      "|   ae.layers                                       |   6.0M               |\n",
      "|    ae.layers.0                                    |    67.5K             |\n",
      "|    ae.layers.1                                    |    98.7K             |\n",
      "|    ae.layers.2                                    |    0.3M              |\n",
      "|    ae.layers.3                                    |    1.1M              |\n",
      "|    ae.layers.4                                    |    4.4M              |\n",
      "|  transformer_encoder                              |  2.6M                |\n",
      "|   transformer_encoder.encoder_layer               |   1.3M               |\n",
      "|    transformer_encoder.encoder_layer.self_attn    |    0.3M              |\n",
      "|    transformer_encoder.encoder_layer.linear1      |    0.5M              |\n",
      "|    transformer_encoder.encoder_layer.linear2      |    0.5M              |\n",
      "|    transformer_encoder.encoder_layer.norm1        |    0.5K              |\n",
      "|    transformer_encoder.encoder_layer.norm2        |    0.5K              |\n",
      "|   transformer_encoder.transformer_encoder         |   1.3M               |\n",
      "|    transformer_encoder.transformer_encoder.layers |    1.3M              |\n",
      "|   transformer_encoder.linear                      |   9.7K               |\n",
      "|    transformer_encoder.linear.weight              |    (2, 4864)         |\n",
      "|    transformer_encoder.linear.bias                |    (2,)              |\n"
     ]
    }
   ],
   "source": [
    "data_m = torch.randn(1, 19, 24000)\n",
    "out = classifier_m(data_m)\n",
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_m, data_m)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014670ef",
   "metadata": {},
   "source": [
    "### FLOPs for EEGNet_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14fc7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"config_file\", metavar=\"FILE\", help=\"config file\")\n",
    "# parser.add_argument('--run-dir', metavar='DIR', help='run directory')\n",
    "# parser.add_argument('--pdb', action='store_true', help='pdb')\n",
    "args = parser.parse_args(args=['configs/EEGNET.yml'])\n",
    "# args, opts = parser.parse_known_args()\n",
    "# f = 'configs/eeg_pt.yml'\n",
    "with open(args.config_file, 'r') as file:\n",
    "    configs = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07c8321",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EEGNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier_eeg_s \u001b[38;5;241m=\u001b[39m \u001b[43mEEGNet\u001b[49m(signal_length\u001b[38;5;241m=\u001b[39mConfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m],channel\u001b[38;5;241m=\u001b[39mConfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_channels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m                fs\u001b[38;5;241m=\u001b[39mConfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m                 num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(Configs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m                )\n\u001b[1;32m      5\u001b[0m classifier_eeg_s\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EEGNet' is not defined"
     ]
    }
   ],
   "source": [
    "classifier_eeg_s = EEGNet(signal_length=Configs['input_size'],channel=Configs['n_channels'],\n",
    "               fs=Configs['processing']['frequency'],\n",
    "                num_class=len(Configs['dataset']['classes'])\n",
    "               )\n",
    "classifier_eeg_s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b85c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eeg_s = torch.randn(1, 19, 12000)\n",
    "data_eeg_s= data_eeg_s.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26464ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(out, target)\n",
    "# probabilities = torch.softmax(out, dim=1)  # Apply softmax to get probabilities\n",
    "# _, predicted = torch.max(probabilities, 1)  # Get the predicted class\n",
    "flops = FlopCountAnalysis(classifier_eeg_s, data_eeg_s)\n",
    "print(\"FLOPs:\", flops.total())\n",
    "print(parameter_count_table(classifier_eeg_s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python38]",
   "language": "python",
   "name": "conda-env-python38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
