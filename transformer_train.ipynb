{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806273aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torchonn as onn\n",
    "# from torchonn.models import ONNBaseModel\n",
    "# from torchonn.op.mzi_op import project_matrix_to_unitary\n",
    "# from torchonn.layers import MZILinear\n",
    "# from torchonn.models import ONNBaseModel\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc6071",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737da267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Init logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "test_num = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: {test_num:.2f}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e7db",
   "metadata": {},
   "source": [
    "## Load encodered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dir = './data/train_label.csv'\n",
    "train_data_dir = './data/encodered_csv/train/'\n",
    "\n",
    "eval_label_dir = './data/eval_label.csv'\n",
    "eval_data_dir = './data/encodered_csv/eval/'\n",
    "\n",
    "label_dic = {'normal':0, 'abnormal':1}\n",
    "\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/origin_csv/train'\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(label_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = torch.tensor(int(label_dic[self.annotations.iloc[index,1]]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data.t(), label, file_name)\n",
    "\n",
    "# dataset = customDataset(data_dir=data_dir, label_dir=label_dir)\n",
    "train_dataset = customDataset(data_dir=train_data_dir, label_dir=train_label_dir)\n",
    "eval_dataset = customDataset(data_dir=eval_data_dir, label_dir=eval_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 512\n",
    "step = 0\n",
    "init_lr = 1e-2\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \\\n",
    "                                  shuffle=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset=eval_dataset, shuffle=True)\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.debug(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e00a00",
   "metadata": {},
   "source": [
    "## Train transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8cbf0",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f051ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).to('cuda')\n",
    "# out = encoder_layer(src)\n",
    "class transformer_classifier(nn.Module):\n",
    "    def __init__(self, input_size, classes):\n",
    "        super(transformer_classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(input_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.transformer_encoder(x)\n",
    "        z = self.flatten(z)\n",
    "        y = self.linear(z)\n",
    "        return y\n",
    "    \n",
    "classifier = transformer_classifier(256*19, 2).to('cuda')\n",
    "optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.9),lr=init_lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9162b",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260fccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr=: 0.01\n",
      "Epoch: 0, Train Loss: 0.6933990716934204, Val Loss: 0.6923, Accuracy: 52.66%\n",
      "lr=: 0.00995498872932069\n",
      "Epoch: 1, Train Loss: 0.6930138468742371, Val Loss: 0.6909, Accuracy: 53.67%\n",
      "lr=: 0.009909954834128341\n",
      "Epoch: 2, Train Loss: 0.6984460353851318, Val Loss: 0.6998, Accuracy: 46.32%\n",
      "lr=: 0.009864898188698403\n",
      "Epoch: 3, Train Loss: 0.7056841254234314, Val Loss: 0.6906, Accuracy: 53.77%\n",
      "lr=: 0.009819818665965752\n",
      "Epoch: 4, Train Loss: 0.6979411840438843, Val Loss: 0.6993, Accuracy: 46.13%\n",
      "lr=: 0.009774716137503496\n",
      "Epoch: 5, Train Loss: 0.701802134513855, Val Loss: 0.6904, Accuracy: 53.81%\n",
      "lr=: 0.009729590473501306\n",
      "Epoch: 6, Train Loss: 0.6940735578536987, Val Loss: 0.6988, Accuracy: 46.20%\n",
      "lr=: 0.009684441542743309\n",
      "Epoch: 7, Train Loss: 0.6893714666366577, Val Loss: 0.6908, Accuracy: 53.80%\n",
      "lr=: 0.009639269212585509\n",
      "Epoch: 8, Train Loss: 0.6929356455802917, Val Loss: 0.7000, Accuracy: 46.20%\n",
      "lr=: 0.00959407334893272\n",
      "Epoch: 9, Train Loss: 0.6786623597145081, Val Loss: 0.7150, Accuracy: 46.19%\n",
      "lr=: 0.009548853816214998\n",
      "Epoch: 10, Train Loss: 0.6858211159706116, Val Loss: 0.6908, Accuracy: 53.80%\n",
      "lr=: 0.009503610477363587\n",
      "Epoch: 11, Train Loss: 0.6900894641876221, Val Loss: 0.6911, Accuracy: 53.57%\n",
      "lr=: 0.009458343193786321\n",
      "Epoch: 12, Train Loss: 0.6908083558082581, Val Loss: 0.7029, Accuracy: 46.18%\n",
      "lr=: 0.009413051825342491\n",
      "Epoch: 13, Train Loss: 0.7011116147041321, Val Loss: 0.6953, Accuracy: 47.03%\n",
      "lr=: 0.009367736230317175\n",
      "Epoch: 14, Train Loss: 0.6831584572792053, Val Loss: 0.6944, Accuracy: 53.81%\n",
      "lr=: 0.009322396265394996\n",
      "Epoch: 15, Train Loss: 0.6771863698959351, Val Loss: 0.6939, Accuracy: 53.82%\n",
      "lr=: 0.009277031785633282\n",
      "Epoch: 16, Train Loss: 0.6972029209136963, Val Loss: 0.6922, Accuracy: 52.24%\n",
      "lr=: 0.009231642644434657\n",
      "Epoch: 17, Train Loss: 0.6929028034210205, Val Loss: 0.7002, Accuracy: 46.22%\n",
      "lr=: 0.009186228693518995\n",
      "Epoch: 18, Train Loss: 0.7059523463249207, Val Loss: 0.6916, Accuracy: 53.23%\n",
      "lr=: 0.009140789782894745\n",
      "Epoch: 19, Train Loss: 0.6854436993598938, Val Loss: 0.7006, Accuracy: 46.25%\n",
      "lr=: 0.009095325760829623\n",
      "Epoch: 20, Train Loss: 0.6775081157684326, Val Loss: 0.7162, Accuracy: 46.19%\n",
      "lr=: 0.009049836473820612\n",
      "Epoch: 21, Train Loss: 0.69322270154953, Val Loss: 0.7006, Accuracy: 46.21%\n",
      "lr=: 0.00900432176656329\n",
      "Epoch: 22, Train Loss: 0.6998888254165649, Val Loss: 0.6907, Accuracy: 53.75%\n",
      "lr=: 0.008958781481920454\n",
      "Epoch: 23, Train Loss: 0.694881021976471, Val Loss: 0.6959, Accuracy: 46.98%\n",
      "lr=: 0.00891321546089\n",
      "Epoch: 24, Train Loss: 0.6849539279937744, Val Loss: 0.6961, Accuracy: 53.82%\n",
      "lr=: 0.008867623542572074\n",
      "Epoch: 25, Train Loss: 0.6995870471000671, Val Loss: 0.6949, Accuracy: 47.54%\n",
      "lr=: 0.008822005564135439\n",
      "Epoch: 26, Train Loss: 0.6983643174171448, Val Loss: 0.6908, Accuracy: 53.82%\n",
      "lr=: 0.00877636136078306\n",
      "Epoch: 27, Train Loss: 0.6986171007156372, Val Loss: 0.6918, Accuracy: 52.71%\n",
      "lr=: 0.00873069076571686\n",
      "Epoch: 28, Train Loss: 0.6979589462280273, Val Loss: 0.6920, Accuracy: 52.85%\n",
      "lr=: 0.008684993610101652\n",
      "Epoch: 29, Train Loss: 0.7025819420814514, Val Loss: 0.7063, Accuracy: 46.18%\n",
      "lr=: 0.008639269723028191\n",
      "Epoch: 30, Train Loss: 0.6900128126144409, Val Loss: 0.6951, Accuracy: 47.42%\n",
      "lr=: 0.008593518931475346\n",
      "Epoch: 31, Train Loss: 0.6878826022148132, Val Loss: 0.7275, Accuracy: 46.19%\n",
      "lr=: 0.008547741060271343\n",
      "Epoch: 32, Train Loss: 0.6879361867904663, Val Loss: 0.7157, Accuracy: 46.19%\n",
      "lr=: 0.008501935932054073\n",
      "Epoch: 33, Train Loss: 0.7002870440483093, Val Loss: 0.6914, Accuracy: 53.32%\n",
      "lr=: 0.00845610336723042\n",
      "Epoch: 34, Train Loss: 0.6906238794326782, Val Loss: 0.6962, Accuracy: 46.63%\n",
      "lr=: 0.008410243183934575\n",
      "Epoch: 35, Train Loss: 0.70289546251297, Val Loss: 0.6965, Accuracy: 46.41%\n",
      "lr=: 0.008364355197985339\n",
      "Epoch: 36, Train Loss: 0.6940388083457947, Val Loss: 0.6918, Accuracy: 52.90%\n",
      "lr=: 0.008318439222842327\n",
      "Epoch: 37, Train Loss: 0.6958721876144409, Val Loss: 0.6910, Accuracy: 53.57%\n",
      "lr=: 0.008272495069561094\n",
      "Epoch: 38, Train Loss: 0.6905149221420288, Val Loss: 0.7010, Accuracy: 46.21%\n",
      "lr=: 0.008226522546747113\n",
      "Epoch: 39, Train Loss: 0.6862933039665222, Val Loss: 0.7128, Accuracy: 46.19%\n",
      "lr=: 0.008180521460508584\n",
      "Epoch: 40, Train Loss: 0.7173043489456177, Val Loss: 0.6986, Accuracy: 46.24%\n",
      "lr=: 0.008134491614408032\n",
      "Epoch: 41, Train Loss: 0.6890878081321716, Val Loss: 0.6905, Accuracy: 53.77%\n",
      "lr=: 0.008088432809412661\n",
      "Epoch: 42, Train Loss: 0.6900813579559326, Val Loss: 0.6909, Accuracy: 53.81%\n",
      "lr=: 0.008042344843843412\n",
      "Epoch: 43, Train Loss: 0.6958932280540466, Val Loss: 0.7132, Accuracy: 46.18%\n",
      "lr=: 0.007996227513322693\n",
      "Epoch: 44, Train Loss: 0.69297856092453, Val Loss: 0.6903, Accuracy: 53.78%\n",
      "lr=: 0.00795008061072074\n",
      "Epoch: 45, Train Loss: 0.6958346962928772, Val Loss: 0.6926, Accuracy: 51.56%\n",
      "lr=: 0.007903903926100556\n",
      "Epoch: 46, Train Loss: 0.697735071182251, Val Loss: 0.6936, Accuracy: 49.73%\n",
      "lr=: 0.00785769724666137\n",
      "Epoch: 47, Train Loss: 0.7179745435714722, Val Loss: 0.6927, Accuracy: 51.34%\n",
      "lr=: 0.007811460356680608\n",
      "Epoch: 48, Train Loss: 0.678695559501648, Val Loss: 0.7177, Accuracy: 46.18%\n",
      "lr=: 0.007765193037454269\n",
      "Epoch: 49, Train Loss: 0.6964245438575745, Val Loss: 0.6948, Accuracy: 47.90%\n",
      "lr=: 0.007718895067235705\n",
      "Epoch: 50, Train Loss: 0.6948183178901672, Val Loss: 0.6954, Accuracy: 46.93%\n",
      "lr=: 0.007672566221172707\n",
      "Epoch: 51, Train Loss: 0.6916810274124146, Val Loss: 0.7005, Accuracy: 46.20%\n",
      "lr=: 0.007626206271242884\n",
      "Epoch: 52, Train Loss: 0.6918472647666931, Val Loss: 0.7155, Accuracy: 46.19%\n",
      "lr=: 0.007579814986187214\n",
      "Epoch: 53, Train Loss: 0.6888424754142761, Val Loss: 0.6983, Accuracy: 46.37%\n",
      "lr=: 0.007533392131441786\n",
      "Epoch: 54, Train Loss: 0.6973282098770142, Val Loss: 0.6905, Accuracy: 53.82%\n",
      "lr=: 0.007486937469067589\n",
      "Epoch: 55, Train Loss: 0.6963632106781006, Val Loss: 0.6968, Accuracy: 46.31%\n",
      "lr=: 0.0074404507576783276\n",
      "Epoch: 56, Train Loss: 0.6999935507774353, Val Loss: 0.6935, Accuracy: 49.66%\n",
      "lr=: 0.007393931752366188\n",
      "Epoch: 57, Train Loss: 0.6894258260726929, Val Loss: 0.7216, Accuracy: 46.19%\n",
      "lr=: 0.007347380204625457\n",
      "Epoch: 58, Train Loss: 0.697554349899292, Val Loss: 0.7002, Accuracy: 46.23%\n",
      "lr=: 0.007300795862273964\n",
      "Epoch: 59, Train Loss: 0.6965010762214661, Val Loss: 0.6977, Accuracy: 46.46%\n",
      "lr=: 0.007254178469372199\n",
      "Epoch: 60, Train Loss: 0.6872009038925171, Val Loss: 0.6986, Accuracy: 46.15%\n",
      "lr=: 0.007207527766140102\n",
      "Epoch: 61, Train Loss: 0.6901568174362183, Val Loss: 0.6980, Accuracy: 46.26%\n",
      "lr=: 0.007160843488871357\n",
      "Epoch: 62, Train Loss: 0.6972647905349731, Val Loss: 0.7038, Accuracy: 46.19%\n",
      "lr=: 0.007114125369845178\n",
      "Epoch: 63, Train Loss: 0.6896924376487732, Val Loss: 0.6975, Accuracy: 46.29%\n",
      "lr=: 0.007067373137235416\n",
      "Epoch: 64, Train Loss: 0.676874577999115, Val Loss: 0.7058, Accuracy: 46.19%\n",
      "lr=: 0.00702058651501696\n",
      "Epoch: 65, Train Loss: 0.7172649502754211, Val Loss: 0.7009, Accuracy: 46.19%\n",
      "lr=: 0.006973765222869263\n",
      "Epoch: 66, Train Loss: 0.6886866688728333, Val Loss: 0.7206, Accuracy: 46.19%\n",
      "lr=: 0.006926908976076941\n",
      "Epoch: 67, Train Loss: 0.7105060815811157, Val Loss: 0.6924, Accuracy: 51.92%\n",
      "lr=: 0.006880017485427282\n",
      "Epoch: 68, Train Loss: 0.7059659957885742, Val Loss: 0.6942, Accuracy: 48.91%\n",
      "lr=: 0.0068330904571046016\n",
      "Epoch: 69, Train Loss: 0.6866710186004639, Val Loss: 0.6930, Accuracy: 50.88%\n",
      "lr=: 0.006786127592581251\n",
      "Epoch: 70, Train Loss: 0.6931858062744141, Val Loss: 0.6965, Accuracy: 46.38%\n",
      "lr=: 0.006739128588505219\n",
      "Epoch: 71, Train Loss: 0.6917261481285095, Val Loss: 0.6984, Accuracy: 46.20%\n",
      "lr=: 0.006692093136584149\n",
      "Epoch: 72, Train Loss: 0.6949692368507385, Val Loss: 0.6906, Accuracy: 53.81%\n",
      "lr=: 0.006645020923465635\n",
      "Epoch: 73, Train Loss: 0.6955133080482483, Val Loss: 0.6905, Accuracy: 53.81%\n",
      "lr=: 0.0065979116306136565\n",
      "Epoch: 74, Train Loss: 0.6931635141372681, Val Loss: 0.6965, Accuracy: 46.17%\n",
      "lr=: 0.006550764934180986\n",
      "Epoch: 75, Train Loss: 0.6923086643218994, Val Loss: 0.6908, Accuracy: 53.83%\n",
      "lr=: 0.0065035805048774195\n",
      "Epoch: 76, Train Loss: 0.7596043944358826, Val Loss: 0.6937, Accuracy: 53.81%\n",
      "lr=: 0.006456358007833636\n",
      "Epoch: 77, Train Loss: 0.6958077549934387, Val Loss: 0.6938, Accuracy: 48.38%\n",
      "lr=: 0.006409097102460522\n",
      "Epoch: 78, Train Loss: 0.6970193386077881, Val Loss: 0.6904, Accuracy: 53.81%\n",
      "lr=: 0.006361797442303772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Train Loss: 0.6915127635002136, Val Loss: 0.7013, Accuracy: 46.21%\n",
      "lr=: 0.006314458674893553\n",
      "Epoch: 80, Train Loss: 0.7061914205551147, Val Loss: 0.6980, Accuracy: 46.27%\n",
      "lr=: 0.006267080441589023\n",
      "Epoch: 81, Train Loss: 0.7005483508110046, Val Loss: 0.6980, Accuracy: 46.20%\n",
      "lr=: 0.006219662377417515\n",
      "Epoch: 82, Train Loss: 0.6941155791282654, Val Loss: 0.6926, Accuracy: 51.90%\n",
      "lr=: 0.006172204110908094\n",
      "Epoch: 83, Train Loss: 0.6929608583450317, Val Loss: 0.6948, Accuracy: 47.08%\n",
      "lr=: 0.006124705263919325\n",
      "Epoch: 84, Train Loss: 0.6920835375785828, Val Loss: 0.6968, Accuracy: 46.24%\n",
      "lr=: 0.006077165451460892\n",
      "Epoch: 85, Train Loss: 0.6924874186515808, Val Loss: 0.6961, Accuracy: 46.43%\n",
      "lr=: 0.0060295842815089205\n",
      "Epoch: 86, Train Loss: 0.690008282661438, Val Loss: 0.6954, Accuracy: 46.51%\n",
      "lr=: 0.005981961354814591\n",
      "Epoch: 87, Train Loss: 0.6940969824790955, Val Loss: 0.6905, Accuracy: 53.80%\n",
      "lr=: 0.00593429626470586\n",
      "Epoch: 88, Train Loss: 0.6889764666557312, Val Loss: 0.6966, Accuracy: 46.20%\n",
      "lr=: 0.005886588596881868\n",
      "Epoch: 89, Train Loss: 0.6939828395843506, Val Loss: 0.6910, Accuracy: 53.74%\n",
      "lr=: 0.0058388379291998025\n",
      "Epoch: 90, Train Loss: 0.6843230724334717, Val Loss: 0.7020, Accuracy: 46.19%\n",
      "lr=: 0.0057910438314537635\n",
      "Epoch: 91, Train Loss: 0.6923660039901733, Val Loss: 0.6909, Accuracy: 53.78%\n",
      "lr=: 0.005743205865145341\n",
      "Epoch: 92, Train Loss: 0.6910240650177002, Val Loss: 0.6917, Accuracy: 53.65%\n",
      "lr=: 0.005695323583245457\n",
      "Epoch: 93, Train Loss: 0.698688268661499, Val Loss: 0.6970, Accuracy: 46.47%\n",
      "lr=: 0.005647396529947098\n",
      "Epoch: 94, Train Loss: 0.6803321242332458, Val Loss: 0.7093, Accuracy: 46.19%\n",
      "lr=: 0.005599424240408453\n",
      "Epoch: 95, Train Loss: 0.6888216137886047, Val Loss: 0.6979, Accuracy: 46.19%\n",
      "lr=: 0.0055514062404860365\n",
      "Epoch: 96, Train Loss: 0.69068443775177, Val Loss: 0.6965, Accuracy: 46.31%\n",
      "lr=: 0.005503342046457278\n",
      "Epoch: 97, Train Loss: 0.6908146142959595, Val Loss: 0.6921, Accuracy: 53.15%\n",
      "lr=: 0.005455231164732058\n",
      "Epoch: 98, Train Loss: 0.6957013607025146, Val Loss: 0.6928, Accuracy: 51.55%\n",
      "lr=: 0.005407073091552649\n",
      "Epoch: 99, Train Loss: 0.6830535531044006, Val Loss: 0.7127, Accuracy: 46.18%\n",
      "lr=: 0.005358867312681466\n",
      "Epoch: 100, Train Loss: 0.6949368119239807, Val Loss: 0.6920, Accuracy: 53.25%\n",
      "lr=: 0.005310613303076005\n",
      "Epoch: 101, Train Loss: 0.6941810846328735, Val Loss: 0.6948, Accuracy: 47.37%\n",
      "lr=: 0.005262310526550319\n",
      "Epoch: 102, Train Loss: 0.6831894516944885, Val Loss: 0.7025, Accuracy: 46.18%\n",
      "lr=: 0.005213958435422312\n",
      "Epoch: 103, Train Loss: 0.6911047697067261, Val Loss: 0.6914, Accuracy: 53.68%\n",
      "lr=: 0.00516555647014613\n",
      "Epoch: 104, Train Loss: 0.6916714310646057, Val Loss: 0.6905, Accuracy: 53.82%\n",
      "lr=: 0.005117104058928822\n",
      "Epoch: 105, Train Loss: 0.7012676000595093, Val Loss: 0.6912, Accuracy: 53.69%\n",
      "lr=: 0.0050686006173304736\n",
      "Epoch: 106, Train Loss: 0.7035139203071594, Val Loss: 0.6948, Accuracy: 46.59%\n",
      "lr=: 0.005020045547846861\n",
      "Epoch: 107, Train Loss: 0.6920315027236938, Val Loss: 0.6940, Accuracy: 48.13%\n",
      "lr=: 0.004971438239473716\n",
      "Epoch: 108, Train Loss: 0.6920071840286255, Val Loss: 0.6971, Accuracy: 46.21%\n",
      "lr=: 0.00492277806725155\n",
      "Epoch: 109, Train Loss: 0.6919953227043152, Val Loss: 0.6965, Accuracy: 46.23%\n",
      "lr=: 0.004874064391789954\n",
      "Epoch: 110, Train Loss: 0.6932232975959778, Val Loss: 0.6942, Accuracy: 47.05%\n",
      "lr=: 0.004825296558770224\n",
      "Epoch: 111, Train Loss: 0.690656304359436, Val Loss: 0.6941, Accuracy: 47.57%\n",
      "lr=: 0.004776473898425048\n",
      "Epoch: 112, Train Loss: 0.6897338628768921, Val Loss: 0.6908, Accuracy: 53.78%\n",
      "lr=: 0.004727595724993942\n",
      "Epoch: 113, Train Loss: 0.6901037693023682, Val Loss: 0.6987, Accuracy: 46.22%\n",
      "lr=: 0.004678661336153\n",
      "Epoch: 114, Train Loss: 0.6939786672592163, Val Loss: 0.6906, Accuracy: 53.80%\n",
      "lr=: 0.004629670012417444\n",
      "Epoch: 115, Train Loss: 0.6959667801856995, Val Loss: 0.6912, Accuracy: 53.79%\n",
      "lr=: 0.0045806210165153325\n",
      "Epoch: 116, Train Loss: 0.6935266852378845, Val Loss: 0.6924, Accuracy: 52.76%\n",
      "lr=: 0.004531513592730678\n",
      "Epoch: 117, Train Loss: 0.6929104328155518, Val Loss: 0.6913, Accuracy: 53.78%\n",
      "lr=: 0.004482346966214095\n",
      "Epoch: 118, Train Loss: 0.694635808467865, Val Loss: 0.6908, Accuracy: 53.79%\n",
      "lr=: 0.004433120342258954\n",
      "Epoch: 119, Train Loss: 0.6971346735954285, Val Loss: 0.6938, Accuracy: 48.32%\n",
      "lr=: 0.0043838329055408696\n",
      "Epoch: 120, Train Loss: 0.6907708048820496, Val Loss: 0.6954, Accuracy: 46.38%\n",
      "lr=: 0.004334483819318182\n",
      "Epoch: 121, Train Loss: 0.692442774772644, Val Loss: 0.6919, Accuracy: 53.69%\n",
      "lr=: 0.004285072224590917\n",
      "Epoch: 122, Train Loss: 0.6924213171005249, Val Loss: 0.6992, Accuracy: 46.22%\n",
      "lr=: 0.004235597239215497\n",
      "Epoch: 123, Train Loss: 0.688197135925293, Val Loss: 0.7060, Accuracy: 46.18%\n",
      "lr=: 0.004186057956972281\n",
      "Epoch: 124, Train Loss: 0.6907262206077576, Val Loss: 0.6956, Accuracy: 46.27%\n",
      "lr=: 0.004136453446582762\n",
      "Epoch: 125, Train Loss: 0.6906382441520691, Val Loss: 0.6995, Accuracy: 46.21%\n",
      "lr=: 0.004086782750672989\n",
      "Epoch: 126, Train Loss: 0.694875955581665, Val Loss: 0.6937, Accuracy: 47.99%\n",
      "lr=: 0.004037044884679515\n",
      "Epoch: 127, Train Loss: 0.6940576434135437, Val Loss: 0.6917, Accuracy: 53.68%\n",
      "lr=: 0.003987238835693844\n",
      "Epoch: 128, Train Loss: 0.7085451483726501, Val Loss: 0.6964, Accuracy: 46.25%\n",
      "lr=: 0.003937363561241022\n",
      "Epoch: 129, Train Loss: 0.6933870911598206, Val Loss: 0.6923, Accuracy: 52.95%\n",
      "lr=: 0.0038874179879876346\n",
      "Epoch: 130, Train Loss: 0.702937126159668, Val Loss: 0.6910, Accuracy: 53.77%\n",
      "lr=: 0.003837401010374062\n",
      "Epoch: 131, Train Loss: 0.6860771775245667, Val Loss: 0.6970, Accuracy: 46.29%\n",
      "lr=: 0.0037873114891653933\n",
      "Epoch: 132, Train Loss: 0.6933859586715698, Val Loss: 0.6938, Accuracy: 47.36%\n",
      "lr=: 0.0037371482499148866\n",
      "Epoch: 133, Train Loss: 0.6938386559486389, Val Loss: 0.6924, Accuracy: 52.82%\n",
      "lr=: 0.0036869100813333197\n",
      "Epoch: 134, Train Loss: 0.688848614692688, Val Loss: 0.6910, Accuracy: 53.76%\n",
      "lr=: 0.003636595733556942\n",
      "Epoch: 135, Train Loss: 0.6980159282684326, Val Loss: 0.6907, Accuracy: 53.80%\n",
      "lr=: 0.003586203916306077\n",
      "Epoch: 136, Train Loss: 0.6931822299957275, Val Loss: 0.6944, Accuracy: 46.98%\n",
      "lr=: 0.003535733296925639\n",
      "Epoch: 137, Train Loss: 0.6939411759376526, Val Loss: 0.6923, Accuracy: 53.09%\n",
      "lr=: 0.0034851824982980034\n",
      "Epoch: 138, Train Loss: 0.6941066384315491, Val Loss: 0.6921, Accuracy: 53.53%\n",
      "lr=: 0.003434550096617719\n",
      "Epoch: 139, Train Loss: 0.6959618926048279, Val Loss: 0.6930, Accuracy: 50.98%\n",
      "lr=: 0.0033838346190164987\n",
      "Epoch: 140, Train Loss: 0.6900862455368042, Val Loss: 0.6976, Accuracy: 46.18%\n",
      "lr=: 0.003333034541025741\n",
      "Epoch: 141, Train Loss: 0.6924179196357727, Val Loss: 0.6959, Accuracy: 46.22%\n",
      "lr=: 0.0032821482838625376\n",
      "Epoch: 142, Train Loss: 0.6958145499229431, Val Loss: 0.6944, Accuracy: 46.74%\n",
      "lr=: 0.0032311742115236114\n",
      "Epoch: 143, Train Loss: 0.6906870007514954, Val Loss: 0.6965, Accuracy: 46.25%\n",
      "lr=: 0.003180110627669995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_loader = DataLoader(dataset=dataset, batch_size=batch_size, \\\n",
    "#                                   shuffle=True)\n",
    "\n",
    "# src = torch.rand(18, 128).to('cuda')\n",
    "# label = torch.tensor([0]).to('cuda')\n",
    "# x_hat = ae.encoder(data)\n",
    "\n",
    "min_loss = 1\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    poly_lr_scheduler(optimizer, init_lr=init_lr, iter=epoch, max_iter=epochs)\n",
    "    for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "#     for batch_index, data in enumerate(train_loader, 0):\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "        y = classifier(data)\n",
    "#         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "        train_loss = criterion(y, target)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "#         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "#     logger.info(f\"Training Loss: {loss}\")\n",
    "    if epoch%1==0:\n",
    "    # Validation loop\n",
    "#         classifier.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "                outputs = classifier(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += target.size(0)  # Total number of samples\n",
    "                correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "\n",
    "        val_loss /= len(eval_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        logger.info(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "                    \n",
    "    torch.save(classifier.state_dict(), './weights/transformer_params_latest.pth')\n",
    "    if train_loss < min_loss:\n",
    "        torch.save(classifier.state_dict(), './weights/transformer_params_best.pth')\n",
    "        min_loss = train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8c0eb",
   "metadata": {},
   "source": [
    "## Save transformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), './weights/transformer_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf706b",
   "metadata": {},
   "source": [
    "### Normalize dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecd04212",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data.reshape((len(train_data), -1)))\n",
    "X_test = scaler.fit_transform(val_data.reshape((len(val_data), -1)))   \n",
    "logger.debug(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765076ec",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "710b9862",
   "metadata": {},
   "source": [
    "def positional_encoding(max_length, d_model, model_type='sinusoidal'):\n",
    "    \"\"\"\n",
    "    Generates positional encodings for a given maximum sequence length and model dimensionality.\n",
    "\n",
    "    Args:\n",
    "        max_length (int): The maximum length of the sequence.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        model_type (str): The type of positional encoding to use. Defaults to 'sinusoidal'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The positional encoding matrix of shape (max_length, d_model).\n",
    "    \"\"\"\n",
    "\n",
    "    if model_type == 'sinusoidal':\n",
    "        pe = np.zeros((max_length, d_model))\n",
    "        position = np.arange(0, max_length, dtype=np.float32).reshape(-1, 1)\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type: {}\".format(model_type))\n",
    "\n",
    "    return pe\n",
    "\n",
    "pe_train = positional_encoding(X_train.shape[0], X_train.shape[1])\n",
    "pe_test = positional_encoding(X_test.shape[0], X_test.shape[1])\n",
    "# Add positional encoding to the signal\n",
    "X_train =  X_train + pe_train # Add corresponding row of pe matrix\n",
    "X_test =  X_test + pe_test\n",
    "\n",
    "# def positional_encoding(max_length, d_model, model_type='sinusoidal'):\n",
    "#     for i, signal in enumerate(signal_dataset):\n",
    "#         if len(signal) <= max_length:\n",
    "#             # Pad shorter signals with zeros\n",
    "#             signal = np.pad(signal, (0, max_length - len(signal)), mode='constant')\n",
    "#         else:\n",
    "#             # Truncate longer signals\n",
    "#             signal = signal[:max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603c3",
   "metadata": {},
   "source": [
    "### Build Optimizer and lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25915e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "predictions_test =  []\n",
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train_1, X_train_2)\n",
    "    predictions_test = model(X_test_1, X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_multiclass(pred_arr,original_arr):\n",
    "    if len(pred_arr)!=len(original_arr):\n",
    "        return False\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    original_arr = original_arr.numpy()\n",
    "    final_pred= []\n",
    "    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n",
    "    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n",
    "    for i in range(len(pred_arr)):\n",
    "        final_pred.append(np.argmax(pred_arr[i]))\n",
    "    final_pred = np.array(final_pred)\n",
    "    count = 0\n",
    "    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n",
    "    for i in range(len(original_arr)):\n",
    "        if final_pred[i] == original_arr[i]:\n",
    "            count+=1\n",
    "    return count/len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_accuracy_multiclass(predictions_train.cpu(),y_train.cpu())\n",
    "test_acc  = get_accuracy_multiclass(predictions_test.cpu(),y_test.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training Accuracy: {round(train_acc*100,3)}\")\n",
    "logger.info(f\"Test Accuracy: {round(test_acc*100,3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "1169px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
